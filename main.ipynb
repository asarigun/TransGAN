{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtLiiWFaZlUb"
   },
   "source": [
    "# Paper Information\n",
    "\n",
    "\n",
    "Paper: TransGAN: Two Transformers Can Make One Strong GAN, CVPR 2021, https://arxiv.org/abs/2102.07074\n",
    "\n",
    "Authors: Yifan Jiang, Shiyu Chang, Zhangyang Wang\n",
    "\n",
    "Code Author: Ahmet Sarıgün & Dursun Bekci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUv7BXE1ZrWc"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yodGdGl9ZIqV"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from models import *\n",
    "from utils import * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpMhUguJaL4o"
   },
   "source": [
    "## Training & Saving a Model for CIFAR-10\n",
    "\n",
    "Although in the paper, this implementation claimed as TransGAN-S which is small relatively, it needs more computational power and 1 epoch takes nearly 7 minutes in GPU for CIFAR-10. Therefore, it is recommended to train the model in GPU for CIFAR-10 dataset. When training model on GPU, it may take one day to train. That's why, in this notebook we will mainly focus on MNIST dataset. If you want to know more details of implementation of the model in CIFAR-10, you can look at ```./cifar``` folder and there is a brief explanation about the re-implementation at README.md under the ```./cifar```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4NWSK8XaP8L"
   },
   "source": [
    "## Hyperparameters for MNIST\n",
    "\n",
    "This is the hyperparameters that has been used when training on MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "foVwSWqtaM3S"
   },
   "outputs": [],
   "source": [
    "# training hyperparameters given by code author\n",
    "\n",
    "lr_gen = 0.001 #Learning rate for generator\n",
    "lr_dis = 0.001 #Learning rate for discriminator\n",
    "latent_dim = 128 #Latent dimension\n",
    "gener_batch_size = 60 #Batch size for generator\n",
    "dis_batch_size = 30 #Batch size for discriminator\n",
    "epoch = 200 #Number of epoch\n",
    "weight_decay = 1e-4 #Weight decay\n",
    "drop_rate = 0.5 #dropout\n",
    "\n",
    "# architecture details by authors\n",
    "image_size = 28 #H,W size of image for discriminator\n",
    "initial_size = 7 #Initial size for generator\n",
    "patch_size = 14 #Patch size for generated image\n",
    "num_classes = 1 #Number of classes for discriminator \n",
    "output_dir = 'checkpoint' #saved model path\n",
    "dim = 128 #Embedding dimension \n",
    "depth = 1 #depth for transformers encoder block for discriminator\n",
    "depth1 = 1 #depth for first transformers encoder block-set which is after MLP block for generator\n",
    "depth2 = 1 #depth for second transformers encoder block-set which is after first encoder block for generator\n",
    "depth3 = 1 #depth for third transformers encoder block-set which is after second encoder block for generator\n",
    "heads = 1 #head for attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixr8eOS8aV_u"
   },
   "source": [
    "## Training & Saving Model for MNIST\n",
    "Although the model size and dataset is too small compared to paper that has experimented on big dataset and models, training takes too long even for one epoch for this dataset and model. Hence, please use GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t32za8HaU3k",
    "outputId": "dde46cb6-3b00-4ebb-8cd0-f1146f7d13ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/utils.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(m.weight.data, 1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (patches): ImgPatches(\n",
       "    (patch_embed): Conv2d(1, 128, kernel_size=(14, 14), stride=(14, 14))\n",
       "  )\n",
       "  (droprate): Dropout(p=0.5, inplace=False)\n",
       "  (TransfomerEncoder): TransformerEncoder(\n",
       "    (Encoder_Blocks): ModuleList(\n",
       "      (0): Encoder_Block(\n",
       "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "          (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (out): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (droprateout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)\n",
    "\n",
    "generator= Generator(depth1=1, depth2=1, depth3=1, initial_size=7, dim=128, heads=1, mlp_ratio=4, drop_rate=0.5)\n",
    "generator.to(device)     \n",
    "discriminator = Discriminator(image_size=28, patch_size=14, input_channel=1, num_classes=1, dim=128, depth=1, heads=1, mlp_ratio=4, drop_rate=0.5)\n",
    "discriminator.to(device)\n",
    "generator.apply(inits_weight)\n",
    "discriminator.apply(inits_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D8tnJM5caaj8"
   },
   "outputs": [],
   "source": [
    "optim_gen = optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "                lr=lr_gen, weight_decay= weight_decay)\n",
    "\n",
    "optim_dis = optim.Adam(filter(lambda p: p.requires_grad, discriminator.parameters()),\n",
    "                lr=lr_dis, weight_decay=weight_decay)\n",
    "\n",
    "writer=SummaryWriter()\n",
    "writer_dict = {'writer':writer}\n",
    "writer_dict[\"train_global_steps\"]=0\n",
    "writer_dict[\"valid_global_steps\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aNL7aPvsad0s"
   },
   "outputs": [],
   "source": [
    "def train(generator, discriminator, optim_gen, optim_dis,\n",
    "        epoch, writer,img_size=28, latent_dim = 384,\n",
    "        gener_batch_size=60,device=\"cpu\"):\n",
    "\n",
    "\n",
    "    writer = writer_dict['writer']\n",
    "    gen_step = 0\n",
    "\n",
    "    generator = generator.train()\n",
    "    discriminator = discriminator.train()\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=30, shuffle=True)\n",
    "\n",
    "    for index, (img, _) in enumerate(train_loader):\n",
    "\n",
    "        global_steps = writer_dict['train_global_steps']\n",
    "\n",
    "        real_imgs = img.type(torch.cuda.FloatTensor)\n",
    "        noise = torch.cuda.FloatTensor(np.random.normal(0, 1, (img.shape[0], latent_dim)))\n",
    "\n",
    "        optim_dis.zero_grad()\n",
    "        real_valid=discriminator(real_imgs)\n",
    "        fake_imgs = generator(noise).detach()\n",
    "\n",
    "        assert fake_imgs.size() == real_imgs.size(), f\"fake_imgs.size(): {fake_imgs.size()} real_imgs.size(): {real_imgs.size()}\"\n",
    "\n",
    "        fake_valid = discriminator(fake_imgs)\n",
    "\n",
    "        loss_dis = torch.mean(nn.ReLU(inplace=True)(1.0 - real_valid)).to(device) + torch.mean(nn.ReLU(inplace=True)(1 + fake_valid)).to(device)\n",
    "\n",
    "        loss_dis.backward()\n",
    "        optim_dis.step()\n",
    "\n",
    "        writer.add_scalar(\"loss_dis\", loss_dis.item(), global_steps)\n",
    "\n",
    "        if global_steps % 5 == 0:\n",
    "\n",
    "            optim_gen.zero_grad()\n",
    "\n",
    "            gener_noise = torch.cuda.FloatTensor(np.random.normal(0, 1, (gener_batch_size, latent_dim)))\n",
    "\n",
    "            generated_imgs= generator(gener_noise)\n",
    "            fake_valid = discriminator(generated_imgs)\n",
    "\n",
    "            gener_loss = -torch.mean(fake_valid).to(device)\n",
    "            gener_loss.backward()\n",
    "            optim_gen.step()\n",
    "            writer.add_scalar(\"gener_loss\", gener_loss.item(), global_steps)\n",
    "\n",
    "            gen_step += 1\n",
    "\n",
    "\n",
    "        if gen_step and index % 50 == 0:\n",
    "            sample_imgs = generated_imgs[:25]\n",
    "            img_grid = make_grid(sample_imgs, nrow=5, normalize=True, scale_each=True)\n",
    "            save_image(sample_imgs, f'generated_images/generated_img_{epoch}_{index % len(train_loader)}.jpg', nrow=5, normalize=True, scale_each=True)            \n",
    "            tqdm.write(\"[Epoch %d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" %\n",
    "                (epoch+1, index % len(train_loader)+50, len(train_loader), loss_dis.item(), gener_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "atJuZquMajqf"
   },
   "outputs": [],
   "source": [
    "def validate(generator, writer_dict):\n",
    "\n",
    "\n",
    "\n",
    "        writer = writer_dict['writer']\n",
    "        global_steps = writer_dict['valid_global_steps']\n",
    "\n",
    "        generator = generator.eval()\n",
    "        writer_dict['valid_global_steps'] = global_steps + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIKB0CHHalcL",
    "outputId": "a6488e7a-c375-46f5-edb3-8f96f5652989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] [Batch 50/2000] [D loss: 1.862579] [G loss: -0.103841]\n",
      "[Epoch 1] [Batch 100/2000] [D loss: 1.939610] [G loss: -0.155793]\n",
      "[Epoch 1] [Batch 150/2000] [D loss: 1.880374] [G loss: -0.136092]\n",
      "[Epoch 1] [Batch 200/2000] [D loss: 1.985547] [G loss: -0.080095]\n",
      "[Epoch 1] [Batch 250/2000] [D loss: 1.929414] [G loss: -0.030376]\n",
      "[Epoch 1] [Batch 300/2000] [D loss: 2.034805] [G loss: 0.017664]\n",
      "[Epoch 1] [Batch 350/2000] [D loss: 1.971889] [G loss: 0.109574]\n",
      "[Epoch 1] [Batch 400/2000] [D loss: 2.040691] [G loss: -0.254877]\n",
      "[Epoch 1] [Batch 450/2000] [D loss: 2.233412] [G loss: 0.133916]\n",
      "[Epoch 1] [Batch 500/2000] [D loss: 2.450507] [G loss: -0.632127]\n",
      "[Epoch 1] [Batch 550/2000] [D loss: 1.878941] [G loss: 0.043961]\n",
      "[Epoch 1] [Batch 600/2000] [D loss: 2.232890] [G loss: 0.201755]\n",
      "[Epoch 1] [Batch 650/2000] [D loss: 1.976163] [G loss: 0.257815]\n",
      "[Epoch 1] [Batch 700/2000] [D loss: 1.846442] [G loss: 0.075465]\n",
      "[Epoch 1] [Batch 750/2000] [D loss: 2.082432] [G loss: 0.040188]\n",
      "[Epoch 1] [Batch 800/2000] [D loss: 2.086578] [G loss: -0.010282]\n",
      "[Epoch 1] [Batch 850/2000] [D loss: 1.941538] [G loss: 0.004528]\n",
      "[Epoch 1] [Batch 900/2000] [D loss: 1.967415] [G loss: 0.016091]\n",
      "[Epoch 1] [Batch 950/2000] [D loss: 1.878189] [G loss: 0.062402]\n",
      "[Epoch 1] [Batch 1000/2000] [D loss: 1.806593] [G loss: 0.110069]\n",
      "[Epoch 1] [Batch 1050/2000] [D loss: 1.978651] [G loss: -0.165420]\n",
      "[Epoch 1] [Batch 1100/2000] [D loss: 2.039936] [G loss: -0.566357]\n",
      "[Epoch 1] [Batch 1150/2000] [D loss: 1.750662] [G loss: 0.308406]\n",
      "[Epoch 1] [Batch 1200/2000] [D loss: 1.147044] [G loss: 0.683703]\n",
      "[Epoch 1] [Batch 1250/2000] [D loss: 2.260819] [G loss: 0.023539]\n",
      "[Epoch 1] [Batch 1300/2000] [D loss: 1.911005] [G loss: -0.015678]\n",
      "[Epoch 1] [Batch 1350/2000] [D loss: 2.039446] [G loss: 0.239003]\n",
      "[Epoch 1] [Batch 1400/2000] [D loss: 1.953952] [G loss: 0.305608]\n",
      "[Epoch 1] [Batch 1450/2000] [D loss: 2.026863] [G loss: -0.081968]\n",
      "[Epoch 1] [Batch 1500/2000] [D loss: 2.071270] [G loss: 0.022407]\n",
      "[Epoch 1] [Batch 1550/2000] [D loss: 1.793693] [G loss: 0.031958]\n",
      "[Epoch 1] [Batch 1600/2000] [D loss: 2.246963] [G loss: 0.083313]\n",
      "[Epoch 1] [Batch 1650/2000] [D loss: 1.952210] [G loss: -0.059669]\n",
      "[Epoch 1] [Batch 1700/2000] [D loss: 2.010642] [G loss: -0.093495]\n",
      "[Epoch 1] [Batch 1750/2000] [D loss: 1.962308] [G loss: -0.144765]\n",
      "[Epoch 1] [Batch 1800/2000] [D loss: 1.975959] [G loss: -0.083737]\n",
      "[Epoch 1] [Batch 1850/2000] [D loss: 2.074034] [G loss: -0.061080]\n",
      "[Epoch 1] [Batch 1900/2000] [D loss: 1.950604] [G loss: -0.073273]\n",
      "[Epoch 1] [Batch 1950/2000] [D loss: 1.949438] [G loss: -0.203003]\n",
      "[Epoch 1] [Batch 2000/2000] [D loss: 2.160541] [G loss: -0.308188]\n",
      "[Epoch 2] [Batch 50/2000] [D loss: 1.985997] [G loss: -0.311581]\n",
      "[Epoch 2] [Batch 100/2000] [D loss: 1.828513] [G loss: -0.330668]\n",
      "[Epoch 2] [Batch 150/2000] [D loss: 2.013456] [G loss: -0.261103]\n",
      "[Epoch 2] [Batch 200/2000] [D loss: 1.990418] [G loss: -0.136875]\n",
      "[Epoch 2] [Batch 250/2000] [D loss: 1.973990] [G loss: 0.151676]\n",
      "[Epoch 2] [Batch 300/2000] [D loss: 2.033498] [G loss: 0.183914]\n",
      "[Epoch 2] [Batch 350/2000] [D loss: 2.064780] [G loss: 0.026386]\n",
      "[Epoch 2] [Batch 400/2000] [D loss: 1.888824] [G loss: -0.121781]\n",
      "[Epoch 2] [Batch 450/2000] [D loss: 1.972730] [G loss: -0.082779]\n",
      "[Epoch 2] [Batch 500/2000] [D loss: 2.003535] [G loss: -0.100736]\n",
      "[Epoch 2] [Batch 550/2000] [D loss: 2.091975] [G loss: -0.018531]\n",
      "[Epoch 2] [Batch 600/2000] [D loss: 2.019924] [G loss: -0.148315]\n",
      "[Epoch 2] [Batch 650/2000] [D loss: 1.938151] [G loss: -0.014331]\n",
      "[Epoch 2] [Batch 700/2000] [D loss: 2.124614] [G loss: 0.118192]\n",
      "[Epoch 2] [Batch 750/2000] [D loss: 1.955010] [G loss: -0.079561]\n",
      "[Epoch 2] [Batch 800/2000] [D loss: 2.122828] [G loss: 0.114497]\n",
      "[Epoch 2] [Batch 850/2000] [D loss: 1.925911] [G loss: 0.022761]\n",
      "[Epoch 2] [Batch 900/2000] [D loss: 1.983475] [G loss: -0.019338]\n",
      "[Epoch 2] [Batch 950/2000] [D loss: 2.009987] [G loss: 0.012002]\n",
      "[Epoch 2] [Batch 1000/2000] [D loss: 1.967623] [G loss: -0.058994]\n",
      "[Epoch 2] [Batch 1050/2000] [D loss: 1.932525] [G loss: 0.069356]\n",
      "[Epoch 2] [Batch 1100/2000] [D loss: 2.088630] [G loss: 0.099883]\n",
      "[Epoch 2] [Batch 1150/2000] [D loss: 1.874532] [G loss: -0.002401]\n",
      "[Epoch 2] [Batch 1200/2000] [D loss: 1.856296] [G loss: -0.060416]\n",
      "[Epoch 2] [Batch 1250/2000] [D loss: 2.141655] [G loss: -0.074698]\n",
      "[Epoch 2] [Batch 1300/2000] [D loss: 2.008539] [G loss: -0.033667]\n",
      "[Epoch 2] [Batch 1350/2000] [D loss: 2.079315] [G loss: -0.034210]\n",
      "[Epoch 2] [Batch 1400/2000] [D loss: 1.811942] [G loss: 0.009435]\n",
      "[Epoch 2] [Batch 1450/2000] [D loss: 1.933972] [G loss: -0.045985]\n",
      "[Epoch 2] [Batch 1500/2000] [D loss: 2.196581] [G loss: -0.080519]\n",
      "[Epoch 2] [Batch 1550/2000] [D loss: 2.067327] [G loss: -0.042056]\n",
      "[Epoch 2] [Batch 1600/2000] [D loss: 1.981580] [G loss: -0.049317]\n",
      "[Epoch 2] [Batch 1650/2000] [D loss: 2.009245] [G loss: -0.105909]\n",
      "[Epoch 2] [Batch 1700/2000] [D loss: 2.211402] [G loss: -0.045102]\n",
      "[Epoch 2] [Batch 1750/2000] [D loss: 1.762622] [G loss: -0.003779]\n",
      "[Epoch 2] [Batch 1800/2000] [D loss: 2.049261] [G loss: 0.156959]\n",
      "[Epoch 2] [Batch 1850/2000] [D loss: 1.993717] [G loss: 0.100177]\n",
      "[Epoch 2] [Batch 1900/2000] [D loss: 2.021240] [G loss: 0.175830]\n",
      "[Epoch 2] [Batch 1950/2000] [D loss: 2.015918] [G loss: 0.182847]\n",
      "[Epoch 2] [Batch 2000/2000] [D loss: 2.088615] [G loss: 0.061516]\n",
      "[Epoch 3] [Batch 50/2000] [D loss: 2.102763] [G loss: -0.004330]\n",
      "[Epoch 3] [Batch 100/2000] [D loss: 1.871854] [G loss: -0.037445]\n",
      "[Epoch 3] [Batch 150/2000] [D loss: 2.199542] [G loss: -0.065749]\n",
      "[Epoch 3] [Batch 200/2000] [D loss: 2.033759] [G loss: 0.055596]\n",
      "[Epoch 3] [Batch 250/2000] [D loss: 2.089305] [G loss: 0.007336]\n",
      "[Epoch 3] [Batch 300/2000] [D loss: 2.066363] [G loss: 0.179464]\n",
      "[Epoch 3] [Batch 350/2000] [D loss: 2.123497] [G loss: -0.065361]\n",
      "[Epoch 3] [Batch 400/2000] [D loss: 2.202623] [G loss: 0.111666]\n",
      "[Epoch 3] [Batch 450/2000] [D loss: 1.814320] [G loss: -0.153116]\n",
      "[Epoch 3] [Batch 500/2000] [D loss: 1.989103] [G loss: 0.030376]\n",
      "[Epoch 3] [Batch 550/2000] [D loss: 2.057840] [G loss: 0.026401]\n",
      "[Epoch 3] [Batch 600/2000] [D loss: 2.212317] [G loss: 0.042387]\n",
      "[Epoch 3] [Batch 650/2000] [D loss: 1.594906] [G loss: -0.020553]\n",
      "[Epoch 3] [Batch 700/2000] [D loss: 2.029016] [G loss: -0.088736]\n",
      "[Epoch 3] [Batch 750/2000] [D loss: 1.997235] [G loss: 0.037319]\n",
      "[Epoch 3] [Batch 800/2000] [D loss: 2.060299] [G loss: 0.114257]\n",
      "[Epoch 3] [Batch 850/2000] [D loss: 2.261273] [G loss: 0.145654]\n",
      "[Epoch 3] [Batch 900/2000] [D loss: 2.162394] [G loss: 0.194805]\n",
      "[Epoch 3] [Batch 950/2000] [D loss: 2.030337] [G loss: -0.058392]\n",
      "[Epoch 3] [Batch 1000/2000] [D loss: 1.797129] [G loss: -0.211686]\n",
      "[Epoch 3] [Batch 1050/2000] [D loss: 2.234737] [G loss: 0.073570]\n",
      "[Epoch 3] [Batch 1100/2000] [D loss: 1.785429] [G loss: 0.031616]\n",
      "[Epoch 3] [Batch 1150/2000] [D loss: 1.867383] [G loss: -0.041341]\n",
      "[Epoch 3] [Batch 1200/2000] [D loss: 1.779731] [G loss: 0.066924]\n",
      "[Epoch 3] [Batch 1250/2000] [D loss: 1.851906] [G loss: -0.399238]\n",
      "[Epoch 3] [Batch 1300/2000] [D loss: 1.987695] [G loss: -0.328597]\n",
      "[Epoch 3] [Batch 1350/2000] [D loss: 1.884707] [G loss: -0.095091]\n",
      "[Epoch 3] [Batch 1400/2000] [D loss: 1.350551] [G loss: 0.807136]\n",
      "[Epoch 3] [Batch 1450/2000] [D loss: 1.593119] [G loss: 0.552505]\n",
      "[Epoch 3] [Batch 1500/2000] [D loss: 0.821109] [G loss: 0.530001]\n",
      "[Epoch 3] [Batch 1550/2000] [D loss: 0.864464] [G loss: 1.094337]\n",
      "[Epoch 3] [Batch 1600/2000] [D loss: 1.903949] [G loss: 0.264065]\n",
      "[Epoch 3] [Batch 1650/2000] [D loss: 2.226266] [G loss: -0.283526]\n",
      "[Epoch 3] [Batch 1700/2000] [D loss: 2.411582] [G loss: -0.954530]\n",
      "[Epoch 3] [Batch 1750/2000] [D loss: 1.810971] [G loss: -0.257869]\n",
      "[Epoch 3] [Batch 1800/2000] [D loss: 1.759267] [G loss: 0.299085]\n",
      "[Epoch 3] [Batch 1850/2000] [D loss: 2.179298] [G loss: 0.174098]\n",
      "[Epoch 3] [Batch 1900/2000] [D loss: 2.047939] [G loss: -0.093732]\n",
      "[Epoch 3] [Batch 1950/2000] [D loss: 1.722160] [G loss: -0.001607]\n",
      "[Epoch 3] [Batch 2000/2000] [D loss: 1.382713] [G loss: 0.237915]\n",
      "[Epoch 4] [Batch 50/2000] [D loss: 2.427802] [G loss: -0.049746]\n",
      "[Epoch 4] [Batch 100/2000] [D loss: 2.083763] [G loss: 0.053301]\n",
      "[Epoch 4] [Batch 150/2000] [D loss: 1.941601] [G loss: 0.299857]\n",
      "[Epoch 4] [Batch 200/2000] [D loss: 2.163308] [G loss: 0.145850]\n",
      "[Epoch 4] [Batch 250/2000] [D loss: 2.125256] [G loss: 0.071657]\n",
      "[Epoch 4] [Batch 300/2000] [D loss: 1.909367] [G loss: 0.215489]\n",
      "[Epoch 4] [Batch 350/2000] [D loss: 1.951043] [G loss: -0.041502]\n",
      "[Epoch 4] [Batch 400/2000] [D loss: 1.768753] [G loss: -0.253829]\n",
      "[Epoch 4] [Batch 450/2000] [D loss: 0.930345] [G loss: 0.253880]\n",
      "[Epoch 4] [Batch 500/2000] [D loss: 1.668375] [G loss: 0.146819]\n",
      "[Epoch 4] [Batch 550/2000] [D loss: 1.895468] [G loss: -0.322774]\n",
      "[Epoch 4] [Batch 600/2000] [D loss: 2.063691] [G loss: 0.301901]\n",
      "[Epoch 4] [Batch 650/2000] [D loss: 1.995052] [G loss: 0.303041]\n",
      "[Epoch 4] [Batch 700/2000] [D loss: 1.918766] [G loss: 0.307313]\n",
      "[Epoch 4] [Batch 750/2000] [D loss: 1.989284] [G loss: 0.290740]\n",
      "[Epoch 4] [Batch 800/2000] [D loss: 1.715825] [G loss: 0.226901]\n",
      "[Epoch 4] [Batch 850/2000] [D loss: 1.957992] [G loss: 0.285050]\n",
      "[Epoch 4] [Batch 900/2000] [D loss: 2.014910] [G loss: 0.373630]\n",
      "[Epoch 4] [Batch 950/2000] [D loss: 1.964285] [G loss: 0.390961]\n",
      "[Epoch 4] [Batch 1000/2000] [D loss: 1.998524] [G loss: 0.376851]\n",
      "[Epoch 4] [Batch 1050/2000] [D loss: 1.969750] [G loss: 0.302623]\n",
      "[Epoch 4] [Batch 1100/2000] [D loss: 1.946940] [G loss: 0.377267]\n",
      "[Epoch 4] [Batch 1150/2000] [D loss: 1.894071] [G loss: 0.465070]\n",
      "[Epoch 4] [Batch 1200/2000] [D loss: 2.000984] [G loss: -0.018299]\n",
      "[Epoch 4] [Batch 1250/2000] [D loss: 2.058634] [G loss: 0.149206]\n",
      "[Epoch 4] [Batch 1300/2000] [D loss: 2.018404] [G loss: 0.076704]\n",
      "[Epoch 4] [Batch 1350/2000] [D loss: 2.079699] [G loss: 0.037430]\n",
      "[Epoch 4] [Batch 1400/2000] [D loss: 1.957027] [G loss: 0.160353]\n",
      "[Epoch 4] [Batch 1450/2000] [D loss: 2.170402] [G loss: 0.125548]\n",
      "[Epoch 4] [Batch 1500/2000] [D loss: 2.227880] [G loss: 0.228531]\n",
      "[Epoch 4] [Batch 1550/2000] [D loss: 2.129322] [G loss: 0.141537]\n",
      "[Epoch 4] [Batch 1600/2000] [D loss: 1.732990] [G loss: 0.062981]\n",
      "[Epoch 4] [Batch 1650/2000] [D loss: 1.838648] [G loss: 0.021641]\n",
      "[Epoch 4] [Batch 1700/2000] [D loss: 1.962049] [G loss: -0.026456]\n",
      "[Epoch 4] [Batch 1750/2000] [D loss: 1.980686] [G loss: -0.079836]\n",
      "[Epoch 4] [Batch 1800/2000] [D loss: 2.074623] [G loss: -0.023730]\n",
      "[Epoch 4] [Batch 1850/2000] [D loss: 1.993298] [G loss: 0.041979]\n",
      "[Epoch 4] [Batch 1900/2000] [D loss: 1.934033] [G loss: 0.134999]\n",
      "[Epoch 4] [Batch 1950/2000] [D loss: 2.039785] [G loss: 0.040753]\n",
      "[Epoch 4] [Batch 2000/2000] [D loss: 2.014991] [G loss: -0.005402]\n",
      "[Epoch 5] [Batch 50/2000] [D loss: 2.157960] [G loss: -0.000599]\n",
      "[Epoch 5] [Batch 100/2000] [D loss: 1.869734] [G loss: -0.015048]\n",
      "[Epoch 5] [Batch 150/2000] [D loss: 2.000822] [G loss: 0.105989]\n",
      "[Epoch 5] [Batch 200/2000] [D loss: 2.080074] [G loss: -0.030145]\n",
      "[Epoch 5] [Batch 250/2000] [D loss: 1.938764] [G loss: -0.064153]\n",
      "[Epoch 5] [Batch 300/2000] [D loss: 1.823680] [G loss: -0.042676]\n",
      "[Epoch 5] [Batch 350/2000] [D loss: 2.085027] [G loss: -0.109779]\n",
      "[Epoch 5] [Batch 400/2000] [D loss: 2.142626] [G loss: -0.139867]\n",
      "[Epoch 5] [Batch 450/2000] [D loss: 2.214717] [G loss: -0.087345]\n",
      "[Epoch 5] [Batch 500/2000] [D loss: 2.127464] [G loss: 0.080328]\n",
      "[Epoch 5] [Batch 550/2000] [D loss: 1.731414] [G loss: 0.027188]\n",
      "[Epoch 5] [Batch 600/2000] [D loss: 2.260083] [G loss: -0.109378]\n",
      "[Epoch 5] [Batch 650/2000] [D loss: 1.809020] [G loss: -0.116375]\n",
      "[Epoch 5] [Batch 700/2000] [D loss: 2.104674] [G loss: 0.076608]\n",
      "[Epoch 5] [Batch 750/2000] [D loss: 2.072074] [G loss: 0.042840]\n",
      "[Epoch 5] [Batch 800/2000] [D loss: 1.970804] [G loss: -0.031686]\n",
      "[Epoch 5] [Batch 850/2000] [D loss: 1.794569] [G loss: -0.021876]\n",
      "[Epoch 5] [Batch 900/2000] [D loss: 1.925303] [G loss: -0.079203]\n",
      "[Epoch 5] [Batch 950/2000] [D loss: 2.088213] [G loss: -0.024801]\n",
      "[Epoch 5] [Batch 1000/2000] [D loss: 2.002907] [G loss: -0.095194]\n",
      "[Epoch 5] [Batch 1050/2000] [D loss: 1.863931] [G loss: 0.042921]\n",
      "[Epoch 5] [Batch 1100/2000] [D loss: 2.263536] [G loss: 0.108856]\n",
      "[Epoch 5] [Batch 1150/2000] [D loss: 1.954609] [G loss: 0.030864]\n",
      "[Epoch 5] [Batch 1200/2000] [D loss: 2.075137] [G loss: 0.129563]\n",
      "[Epoch 5] [Batch 1250/2000] [D loss: 2.163621] [G loss: 0.096897]\n",
      "[Epoch 5] [Batch 1300/2000] [D loss: 1.952929] [G loss: 0.058591]\n",
      "[Epoch 5] [Batch 1350/2000] [D loss: 1.997975] [G loss: 0.043435]\n",
      "[Epoch 5] [Batch 1400/2000] [D loss: 2.011513] [G loss: 0.030323]\n",
      "[Epoch 5] [Batch 1450/2000] [D loss: 2.116484] [G loss: 0.032511]\n",
      "[Epoch 5] [Batch 1500/2000] [D loss: 1.970949] [G loss: 0.113577]\n",
      "[Epoch 5] [Batch 1550/2000] [D loss: 1.986547] [G loss: 0.021696]\n",
      "[Epoch 5] [Batch 1600/2000] [D loss: 2.022665] [G loss: 0.016786]\n",
      "[Epoch 5] [Batch 1650/2000] [D loss: 2.040741] [G loss: 0.005461]\n",
      "[Epoch 5] [Batch 1700/2000] [D loss: 2.012558] [G loss: -0.205266]\n",
      "[Epoch 5] [Batch 1750/2000] [D loss: 1.835045] [G loss: -0.149604]\n",
      "[Epoch 5] [Batch 1800/2000] [D loss: 2.024192] [G loss: -0.183055]\n",
      "[Epoch 5] [Batch 1850/2000] [D loss: 2.111793] [G loss: -0.014097]\n",
      "[Epoch 5] [Batch 1900/2000] [D loss: 1.944725] [G loss: -0.043013]\n",
      "[Epoch 5] [Batch 1950/2000] [D loss: 2.029456] [G loss: 0.022198]\n",
      "[Epoch 5] [Batch 2000/2000] [D loss: 2.280854] [G loss: -0.122010]\n",
      "[Epoch 6] [Batch 50/2000] [D loss: 1.969520] [G loss: -0.028346]\n",
      "[Epoch 6] [Batch 100/2000] [D loss: 1.956016] [G loss: 0.075724]\n",
      "[Epoch 6] [Batch 150/2000] [D loss: 2.002984] [G loss: 0.465852]\n",
      "[Epoch 6] [Batch 200/2000] [D loss: 2.107156] [G loss: 0.000165]\n",
      "[Epoch 6] [Batch 250/2000] [D loss: 1.833111] [G loss: 0.051519]\n",
      "[Epoch 6] [Batch 300/2000] [D loss: 1.751964] [G loss: 0.243142]\n",
      "[Epoch 6] [Batch 350/2000] [D loss: 1.980169] [G loss: 0.335091]\n",
      "[Epoch 6] [Batch 400/2000] [D loss: 2.189894] [G loss: 0.196544]\n",
      "[Epoch 6] [Batch 450/2000] [D loss: 2.047445] [G loss: 0.367224]\n",
      "[Epoch 6] [Batch 500/2000] [D loss: 2.008564] [G loss: 0.252158]\n",
      "[Epoch 6] [Batch 550/2000] [D loss: 2.029691] [G loss: 0.157555]\n",
      "[Epoch 6] [Batch 600/2000] [D loss: 2.092709] [G loss: 0.093263]\n",
      "[Epoch 6] [Batch 650/2000] [D loss: 2.043663] [G loss: 0.075615]\n",
      "[Epoch 6] [Batch 700/2000] [D loss: 1.943880] [G loss: 0.081154]\n",
      "[Epoch 6] [Batch 750/2000] [D loss: 2.050299] [G loss: 0.047636]\n",
      "[Epoch 6] [Batch 800/2000] [D loss: 1.976508] [G loss: 0.009070]\n",
      "[Epoch 6] [Batch 850/2000] [D loss: 2.178664] [G loss: 0.027052]\n",
      "[Epoch 6] [Batch 900/2000] [D loss: 1.881760] [G loss: -0.014094]\n",
      "[Epoch 6] [Batch 950/2000] [D loss: 2.047686] [G loss: 0.002831]\n",
      "[Epoch 6] [Batch 1000/2000] [D loss: 1.955485] [G loss: -0.029245]\n",
      "[Epoch 6] [Batch 1050/2000] [D loss: 2.048304] [G loss: -0.001752]\n",
      "[Epoch 6] [Batch 1100/2000] [D loss: 1.964978] [G loss: -0.041150]\n",
      "[Epoch 6] [Batch 1150/2000] [D loss: 2.059906] [G loss: -0.088387]\n",
      "[Epoch 6] [Batch 1200/2000] [D loss: 1.953380] [G loss: -0.079516]\n",
      "[Epoch 6] [Batch 1250/2000] [D loss: 1.935720] [G loss: -0.100364]\n",
      "[Epoch 6] [Batch 1300/2000] [D loss: 2.122210] [G loss: -0.034181]\n",
      "[Epoch 6] [Batch 1350/2000] [D loss: 2.084221] [G loss: -0.019279]\n",
      "[Epoch 6] [Batch 1400/2000] [D loss: 1.965317] [G loss: -0.044400]\n",
      "[Epoch 6] [Batch 1450/2000] [D loss: 2.090331] [G loss: 0.056193]\n",
      "[Epoch 6] [Batch 1500/2000] [D loss: 2.025858] [G loss: 0.074221]\n",
      "[Epoch 6] [Batch 1550/2000] [D loss: 2.069994] [G loss: -0.007122]\n",
      "[Epoch 6] [Batch 1600/2000] [D loss: 1.595412] [G loss: 0.114106]\n",
      "[Epoch 6] [Batch 1650/2000] [D loss: 1.903171] [G loss: 0.007192]\n",
      "[Epoch 6] [Batch 1700/2000] [D loss: 1.987349] [G loss: 0.111773]\n",
      "[Epoch 6] [Batch 1750/2000] [D loss: 1.964745] [G loss: 0.050501]\n",
      "[Epoch 6] [Batch 1800/2000] [D loss: 1.972050] [G loss: 0.078700]\n",
      "[Epoch 6] [Batch 1850/2000] [D loss: 1.825177] [G loss: 0.052471]\n",
      "[Epoch 6] [Batch 1900/2000] [D loss: 1.960618] [G loss: -0.028204]\n",
      "[Epoch 6] [Batch 1950/2000] [D loss: 2.139204] [G loss: 0.157579]\n",
      "[Epoch 6] [Batch 2000/2000] [D loss: 2.030088] [G loss: 0.074888]\n",
      "[Epoch 7] [Batch 50/2000] [D loss: 1.791366] [G loss: 0.096520]\n",
      "[Epoch 7] [Batch 100/2000] [D loss: 1.997204] [G loss: 0.081245]\n",
      "[Epoch 7] [Batch 150/2000] [D loss: 1.855183] [G loss: 0.148345]\n",
      "[Epoch 7] [Batch 200/2000] [D loss: 1.830005] [G loss: 0.199639]\n",
      "[Epoch 7] [Batch 250/2000] [D loss: 2.114804] [G loss: 0.085695]\n",
      "[Epoch 7] [Batch 300/2000] [D loss: 2.035563] [G loss: -0.065940]\n",
      "[Epoch 7] [Batch 350/2000] [D loss: 2.004568] [G loss: 0.035779]\n",
      "[Epoch 7] [Batch 400/2000] [D loss: 2.002036] [G loss: 0.211415]\n",
      "[Epoch 7] [Batch 450/2000] [D loss: 1.814922] [G loss: 0.060778]\n",
      "[Epoch 7] [Batch 500/2000] [D loss: 2.051727] [G loss: 0.162708]\n",
      "[Epoch 7] [Batch 550/2000] [D loss: 1.944755] [G loss: -0.039949]\n",
      "[Epoch 7] [Batch 600/2000] [D loss: 1.932267] [G loss: 0.088847]\n",
      "[Epoch 7] [Batch 650/2000] [D loss: 1.834540] [G loss: -0.056112]\n",
      "[Epoch 7] [Batch 700/2000] [D loss: 1.841800] [G loss: 0.055110]\n",
      "[Epoch 7] [Batch 750/2000] [D loss: 1.934118] [G loss: 0.116675]\n",
      "[Epoch 7] [Batch 800/2000] [D loss: 2.033919] [G loss: 0.039479]\n",
      "[Epoch 7] [Batch 850/2000] [D loss: 1.878285] [G loss: 0.022063]\n",
      "[Epoch 7] [Batch 900/2000] [D loss: 2.057397] [G loss: 0.009246]\n",
      "[Epoch 7] [Batch 950/2000] [D loss: 2.001257] [G loss: 0.079135]\n",
      "[Epoch 7] [Batch 1000/2000] [D loss: 2.206115] [G loss: 0.057531]\n",
      "[Epoch 7] [Batch 1050/2000] [D loss: 1.982987] [G loss: 0.097391]\n",
      "[Epoch 7] [Batch 1100/2000] [D loss: 2.142717] [G loss: 0.045493]\n",
      "[Epoch 7] [Batch 1150/2000] [D loss: 2.024709] [G loss: 0.062460]\n",
      "[Epoch 7] [Batch 1200/2000] [D loss: 2.116672] [G loss: 0.140738]\n",
      "[Epoch 7] [Batch 1250/2000] [D loss: 2.218536] [G loss: 0.186915]\n",
      "[Epoch 7] [Batch 1300/2000] [D loss: 2.009801] [G loss: 0.223453]\n",
      "[Epoch 7] [Batch 1350/2000] [D loss: 1.907159] [G loss: 0.096186]\n",
      "[Epoch 7] [Batch 1400/2000] [D loss: 2.014991] [G loss: 0.025240]\n",
      "[Epoch 7] [Batch 1450/2000] [D loss: 1.850567] [G loss: 0.138682]\n",
      "[Epoch 7] [Batch 1500/2000] [D loss: 1.876527] [G loss: 0.077041]\n",
      "[Epoch 7] [Batch 1550/2000] [D loss: 1.987552] [G loss: 0.078357]\n",
      "[Epoch 7] [Batch 1600/2000] [D loss: 1.973321] [G loss: 0.034107]\n",
      "[Epoch 7] [Batch 1650/2000] [D loss: 1.687821] [G loss: 0.077114]\n",
      "[Epoch 7] [Batch 1700/2000] [D loss: 2.244171] [G loss: -0.120315]\n",
      "[Epoch 7] [Batch 1750/2000] [D loss: 2.108714] [G loss: 0.151744]\n",
      "[Epoch 7] [Batch 1800/2000] [D loss: 1.966917] [G loss: 0.165694]\n",
      "[Epoch 7] [Batch 1850/2000] [D loss: 1.701070] [G loss: 0.032053]\n",
      "[Epoch 7] [Batch 1900/2000] [D loss: 2.066894] [G loss: -0.043768]\n",
      "[Epoch 7] [Batch 1950/2000] [D loss: 2.001182] [G loss: 0.002475]\n",
      "[Epoch 7] [Batch 2000/2000] [D loss: 1.937442] [G loss: -0.093034]\n",
      "[Epoch 8] [Batch 50/2000] [D loss: 1.857829] [G loss: -0.045539]\n",
      "[Epoch 8] [Batch 100/2000] [D loss: 1.978426] [G loss: -0.067194]\n",
      "[Epoch 8] [Batch 150/2000] [D loss: 2.026232] [G loss: 0.092447]\n",
      "[Epoch 8] [Batch 200/2000] [D loss: 2.052774] [G loss: 0.070206]\n",
      "[Epoch 8] [Batch 250/2000] [D loss: 1.959723] [G loss: -0.014957]\n",
      "[Epoch 8] [Batch 300/2000] [D loss: 2.047474] [G loss: 0.056900]\n",
      "[Epoch 8] [Batch 350/2000] [D loss: 2.081468] [G loss: -0.068488]\n",
      "[Epoch 8] [Batch 400/2000] [D loss: 2.019794] [G loss: -0.105861]\n",
      "[Epoch 8] [Batch 450/2000] [D loss: 2.116223] [G loss: 0.066672]\n",
      "[Epoch 8] [Batch 500/2000] [D loss: 1.888461] [G loss: 0.164769]\n",
      "[Epoch 8] [Batch 550/2000] [D loss: 2.020033] [G loss: 0.060378]\n",
      "[Epoch 8] [Batch 600/2000] [D loss: 2.102770] [G loss: 0.047835]\n",
      "[Epoch 8] [Batch 650/2000] [D loss: 1.974104] [G loss: -0.008073]\n",
      "[Epoch 8] [Batch 700/2000] [D loss: 1.961780] [G loss: 0.051180]\n",
      "[Epoch 8] [Batch 750/2000] [D loss: 1.857411] [G loss: -0.124360]\n",
      "[Epoch 8] [Batch 800/2000] [D loss: 2.192314] [G loss: -0.146039]\n",
      "[Epoch 8] [Batch 850/2000] [D loss: 2.145023] [G loss: -0.003512]\n",
      "[Epoch 8] [Batch 900/2000] [D loss: 1.916259] [G loss: -0.045381]\n",
      "[Epoch 8] [Batch 950/2000] [D loss: 2.043294] [G loss: -0.113902]\n",
      "[Epoch 8] [Batch 1000/2000] [D loss: 2.253783] [G loss: -0.126317]\n",
      "[Epoch 8] [Batch 1050/2000] [D loss: 1.848914] [G loss: 0.017982]\n",
      "[Epoch 8] [Batch 1100/2000] [D loss: 2.432750] [G loss: 0.039642]\n",
      "[Epoch 8] [Batch 1150/2000] [D loss: 1.290028] [G loss: 0.775456]\n",
      "[Epoch 8] [Batch 1200/2000] [D loss: 1.121527] [G loss: 0.323015]\n",
      "[Epoch 8] [Batch 1250/2000] [D loss: 0.834758] [G loss: 1.024989]\n",
      "[Epoch 8] [Batch 1300/2000] [D loss: 0.346707] [G loss: 1.164408]\n",
      "[Epoch 8] [Batch 1350/2000] [D loss: 0.050029] [G loss: 1.344687]\n",
      "[Epoch 8] [Batch 1400/2000] [D loss: 0.218885] [G loss: 1.568704]\n",
      "[Epoch 8] [Batch 1450/2000] [D loss: 2.134890] [G loss: -0.424124]\n",
      "[Epoch 8] [Batch 1500/2000] [D loss: 1.545599] [G loss: 0.153418]\n",
      "[Epoch 8] [Batch 1550/2000] [D loss: 1.708458] [G loss: 0.520040]\n",
      "[Epoch 8] [Batch 1600/2000] [D loss: 1.487143] [G loss: 0.318437]\n",
      "[Epoch 8] [Batch 1650/2000] [D loss: 1.379107] [G loss: 0.506178]\n",
      "[Epoch 8] [Batch 1700/2000] [D loss: 2.159707] [G loss: -0.014135]\n",
      "[Epoch 8] [Batch 1750/2000] [D loss: 1.461491] [G loss: 0.259320]\n",
      "[Epoch 8] [Batch 1800/2000] [D loss: 2.017534] [G loss: 0.119138]\n",
      "[Epoch 8] [Batch 1850/2000] [D loss: 1.718453] [G loss: 0.160199]\n",
      "[Epoch 8] [Batch 1900/2000] [D loss: 2.273230] [G loss: 0.033072]\n",
      "[Epoch 8] [Batch 1950/2000] [D loss: 1.859216] [G loss: 0.225291]\n",
      "[Epoch 8] [Batch 2000/2000] [D loss: 1.362157] [G loss: 0.159735]\n",
      "[Epoch 9] [Batch 50/2000] [D loss: 1.389682] [G loss: 0.336575]\n",
      "[Epoch 9] [Batch 100/2000] [D loss: 1.576377] [G loss: 0.516743]\n",
      "[Epoch 9] [Batch 150/2000] [D loss: 2.426570] [G loss: 0.206827]\n",
      "[Epoch 9] [Batch 200/2000] [D loss: 1.893985] [G loss: -0.163948]\n",
      "[Epoch 9] [Batch 250/2000] [D loss: 2.097525] [G loss: -0.157603]\n",
      "[Epoch 9] [Batch 300/2000] [D loss: 1.644521] [G loss: -0.167582]\n",
      "[Epoch 9] [Batch 350/2000] [D loss: 1.466722] [G loss: 0.275856]\n",
      "[Epoch 9] [Batch 400/2000] [D loss: 1.947400] [G loss: 0.418010]\n",
      "[Epoch 9] [Batch 450/2000] [D loss: 2.033981] [G loss: 0.422641]\n",
      "[Epoch 9] [Batch 500/2000] [D loss: 1.678549] [G loss: 0.168769]\n",
      "[Epoch 9] [Batch 550/2000] [D loss: 1.659552] [G loss: 0.011123]\n",
      "[Epoch 9] [Batch 600/2000] [D loss: 2.129965] [G loss: -0.505480]\n",
      "[Epoch 9] [Batch 650/2000] [D loss: 1.448769] [G loss: 0.963564]\n",
      "[Epoch 9] [Batch 700/2000] [D loss: 1.855660] [G loss: -0.280817]\n",
      "[Epoch 9] [Batch 750/2000] [D loss: 2.183876] [G loss: -0.465084]\n",
      "[Epoch 9] [Batch 800/2000] [D loss: 1.822315] [G loss: -0.242878]\n",
      "[Epoch 9] [Batch 850/2000] [D loss: 1.857620] [G loss: -0.139117]\n",
      "[Epoch 9] [Batch 900/2000] [D loss: 2.113516] [G loss: -0.184187]\n",
      "[Epoch 9] [Batch 950/2000] [D loss: 1.833816] [G loss: -0.093926]\n",
      "[Epoch 9] [Batch 1000/2000] [D loss: 1.572561] [G loss: 0.401860]\n",
      "[Epoch 9] [Batch 1050/2000] [D loss: 1.666558] [G loss: -0.008484]\n",
      "[Epoch 9] [Batch 1100/2000] [D loss: 1.561177] [G loss: 0.577254]\n",
      "[Epoch 9] [Batch 1150/2000] [D loss: 1.492244] [G loss: 0.764379]\n",
      "[Epoch 9] [Batch 1200/2000] [D loss: 1.791579] [G loss: -0.079493]\n",
      "[Epoch 9] [Batch 1250/2000] [D loss: 2.300100] [G loss: -0.353093]\n",
      "[Epoch 9] [Batch 1300/2000] [D loss: 1.624624] [G loss: 0.241337]\n",
      "[Epoch 9] [Batch 1350/2000] [D loss: 1.702808] [G loss: -0.099198]\n",
      "[Epoch 9] [Batch 1400/2000] [D loss: 1.558321] [G loss: 1.160455]\n",
      "[Epoch 9] [Batch 1450/2000] [D loss: 1.566279] [G loss: -0.263089]\n",
      "[Epoch 9] [Batch 1500/2000] [D loss: 1.283171] [G loss: 0.525203]\n",
      "[Epoch 9] [Batch 1550/2000] [D loss: 0.860279] [G loss: 0.905320]\n",
      "[Epoch 9] [Batch 1600/2000] [D loss: 0.969162] [G loss: 0.374943]\n",
      "[Epoch 9] [Batch 1650/2000] [D loss: 2.043040] [G loss: -0.358983]\n",
      "[Epoch 9] [Batch 1700/2000] [D loss: 2.075186] [G loss: -0.227586]\n",
      "[Epoch 9] [Batch 1750/2000] [D loss: 2.262861] [G loss: -0.874442]\n",
      "[Epoch 9] [Batch 1800/2000] [D loss: 1.300304] [G loss: -0.158410]\n",
      "[Epoch 9] [Batch 1850/2000] [D loss: 1.526790] [G loss: -0.265185]\n",
      "[Epoch 9] [Batch 1900/2000] [D loss: 2.462646] [G loss: 0.100525]\n",
      "[Epoch 9] [Batch 1950/2000] [D loss: 1.827924] [G loss: -0.484444]\n",
      "[Epoch 9] [Batch 2000/2000] [D loss: 2.042696] [G loss: -0.217858]\n",
      "[Epoch 10] [Batch 50/2000] [D loss: 1.701050] [G loss: -0.137232]\n",
      "[Epoch 10] [Batch 100/2000] [D loss: 1.698143] [G loss: -0.122945]\n",
      "[Epoch 10] [Batch 150/2000] [D loss: 1.909184] [G loss: -0.295877]\n",
      "[Epoch 10] [Batch 200/2000] [D loss: 1.769114] [G loss: -0.152983]\n",
      "[Epoch 10] [Batch 250/2000] [D loss: 1.894846] [G loss: -0.239674]\n",
      "[Epoch 10] [Batch 300/2000] [D loss: 1.789723] [G loss: -0.335374]\n",
      "[Epoch 10] [Batch 350/2000] [D loss: 2.412829] [G loss: -0.285270]\n",
      "[Epoch 10] [Batch 400/2000] [D loss: 1.555322] [G loss: -0.151831]\n",
      "[Epoch 10] [Batch 450/2000] [D loss: 1.824039] [G loss: -0.284021]\n",
      "[Epoch 10] [Batch 500/2000] [D loss: 1.571688] [G loss: -0.093836]\n",
      "[Epoch 10] [Batch 550/2000] [D loss: 2.149032] [G loss: -0.334061]\n",
      "[Epoch 10] [Batch 600/2000] [D loss: 1.373623] [G loss: 0.167236]\n",
      "[Epoch 10] [Batch 650/2000] [D loss: 2.104814] [G loss: -0.618334]\n",
      "[Epoch 10] [Batch 700/2000] [D loss: 1.860306] [G loss: -0.395799]\n",
      "[Epoch 10] [Batch 750/2000] [D loss: 1.922110] [G loss: -0.645388]\n",
      "[Epoch 10] [Batch 800/2000] [D loss: 1.711760] [G loss: 0.201536]\n",
      "[Epoch 10] [Batch 850/2000] [D loss: 2.204837] [G loss: 0.050970]\n",
      "[Epoch 10] [Batch 900/2000] [D loss: 1.868261] [G loss: -0.300130]\n",
      "[Epoch 10] [Batch 950/2000] [D loss: 2.188959] [G loss: -0.387284]\n",
      "[Epoch 10] [Batch 1000/2000] [D loss: 1.677819] [G loss: 0.141370]\n",
      "[Epoch 10] [Batch 1050/2000] [D loss: 1.864860] [G loss: 0.145227]\n",
      "[Epoch 10] [Batch 1100/2000] [D loss: 1.332668] [G loss: 0.284407]\n",
      "[Epoch 10] [Batch 1150/2000] [D loss: 1.659692] [G loss: -0.300698]\n",
      "[Epoch 10] [Batch 1200/2000] [D loss: 2.013122] [G loss: -0.435279]\n",
      "[Epoch 10] [Batch 1250/2000] [D loss: 1.217458] [G loss: 0.177910]\n",
      "[Epoch 10] [Batch 1300/2000] [D loss: 1.825321] [G loss: -0.612955]\n",
      "[Epoch 10] [Batch 1350/2000] [D loss: 2.214936] [G loss: -0.065175]\n",
      "[Epoch 10] [Batch 1400/2000] [D loss: 2.525488] [G loss: -0.837946]\n",
      "[Epoch 10] [Batch 1450/2000] [D loss: 1.740611] [G loss: -0.485390]\n",
      "[Epoch 10] [Batch 1500/2000] [D loss: 2.382691] [G loss: -0.371266]\n",
      "[Epoch 10] [Batch 1550/2000] [D loss: 1.721398] [G loss: -0.171281]\n",
      "[Epoch 10] [Batch 1600/2000] [D loss: 1.735430] [G loss: 0.115180]\n",
      "[Epoch 10] [Batch 1650/2000] [D loss: 1.665690] [G loss: 0.015852]\n",
      "[Epoch 10] [Batch 1700/2000] [D loss: 2.060869] [G loss: -0.493909]\n",
      "[Epoch 10] [Batch 1750/2000] [D loss: 1.927703] [G loss: -0.207622]\n",
      "[Epoch 10] [Batch 1800/2000] [D loss: 1.820441] [G loss: -0.065005]\n",
      "[Epoch 10] [Batch 1850/2000] [D loss: 1.868258] [G loss: -0.162978]\n",
      "[Epoch 10] [Batch 1900/2000] [D loss: 1.843472] [G loss: -0.105317]\n",
      "[Epoch 10] [Batch 1950/2000] [D loss: 1.689834] [G loss: -0.202251]\n",
      "[Epoch 10] [Batch 2000/2000] [D loss: 1.413263] [G loss: 0.437204]\n",
      "[Epoch 11] [Batch 50/2000] [D loss: 2.028630] [G loss: -0.278199]\n",
      "[Epoch 11] [Batch 100/2000] [D loss: 1.652540] [G loss: -0.062819]\n",
      "[Epoch 11] [Batch 150/2000] [D loss: 1.874073] [G loss: -0.085430]\n",
      "[Epoch 11] [Batch 200/2000] [D loss: 1.788538] [G loss: -0.175898]\n",
      "[Epoch 11] [Batch 250/2000] [D loss: 1.549373] [G loss: -0.284039]\n",
      "[Epoch 11] [Batch 300/2000] [D loss: 1.896619] [G loss: -0.183293]\n",
      "[Epoch 11] [Batch 350/2000] [D loss: 1.784762] [G loss: 0.047375]\n",
      "[Epoch 11] [Batch 400/2000] [D loss: 1.972469] [G loss: -0.355372]\n",
      "[Epoch 11] [Batch 450/2000] [D loss: 2.284122] [G loss: -0.400772]\n",
      "[Epoch 11] [Batch 500/2000] [D loss: 1.528963] [G loss: 0.156080]\n",
      "[Epoch 11] [Batch 550/2000] [D loss: 2.003337] [G loss: -0.384475]\n",
      "[Epoch 11] [Batch 600/2000] [D loss: 1.714522] [G loss: -0.138019]\n",
      "[Epoch 11] [Batch 650/2000] [D loss: 1.787878] [G loss: -0.571986]\n",
      "[Epoch 11] [Batch 700/2000] [D loss: 1.539331] [G loss: 0.548784]\n",
      "[Epoch 11] [Batch 750/2000] [D loss: 2.001676] [G loss: 0.273372]\n",
      "[Epoch 11] [Batch 800/2000] [D loss: 1.597161] [G loss: -0.031546]\n",
      "[Epoch 11] [Batch 850/2000] [D loss: 1.946406] [G loss: -0.622167]\n",
      "[Epoch 11] [Batch 900/2000] [D loss: 2.032553] [G loss: 0.092153]\n",
      "[Epoch 11] [Batch 950/2000] [D loss: 1.872350] [G loss: -0.575033]\n",
      "[Epoch 11] [Batch 1000/2000] [D loss: 1.799605] [G loss: -0.193772]\n",
      "[Epoch 11] [Batch 1050/2000] [D loss: 2.023956] [G loss: 0.242015]\n",
      "[Epoch 11] [Batch 1100/2000] [D loss: 2.184054] [G loss: -0.592354]\n",
      "[Epoch 11] [Batch 1150/2000] [D loss: 1.674385] [G loss: 0.117429]\n",
      "[Epoch 11] [Batch 1200/2000] [D loss: 2.230399] [G loss: -0.269273]\n",
      "[Epoch 11] [Batch 1250/2000] [D loss: 1.909967] [G loss: -0.362617]\n",
      "[Epoch 11] [Batch 1300/2000] [D loss: 1.589347] [G loss: 0.058980]\n",
      "[Epoch 11] [Batch 1350/2000] [D loss: 2.310289] [G loss: -0.335847]\n",
      "[Epoch 11] [Batch 1400/2000] [D loss: 2.079932] [G loss: 0.185238]\n",
      "[Epoch 11] [Batch 1450/2000] [D loss: 1.724536] [G loss: -0.312077]\n",
      "[Epoch 11] [Batch 1500/2000] [D loss: 1.330147] [G loss: -0.057455]\n",
      "[Epoch 11] [Batch 1550/2000] [D loss: 1.878037] [G loss: -0.380219]\n",
      "[Epoch 11] [Batch 1600/2000] [D loss: 1.903593] [G loss: -0.119047]\n",
      "[Epoch 11] [Batch 1650/2000] [D loss: 2.173945] [G loss: -0.037641]\n",
      "[Epoch 11] [Batch 1700/2000] [D loss: 1.884265] [G loss: 0.040227]\n",
      "[Epoch 11] [Batch 1750/2000] [D loss: 2.005357] [G loss: -0.196316]\n",
      "[Epoch 11] [Batch 1800/2000] [D loss: 2.223159] [G loss: -0.658951]\n",
      "[Epoch 11] [Batch 1850/2000] [D loss: 1.696874] [G loss: -0.027329]\n",
      "[Epoch 11] [Batch 1900/2000] [D loss: 2.022870] [G loss: -0.424059]\n",
      "[Epoch 11] [Batch 1950/2000] [D loss: 2.191558] [G loss: -0.621480]\n",
      "[Epoch 11] [Batch 2000/2000] [D loss: 1.890656] [G loss: -0.363342]\n",
      "[Epoch 12] [Batch 50/2000] [D loss: 1.636284] [G loss: -0.250737]\n",
      "[Epoch 12] [Batch 100/2000] [D loss: 1.830734] [G loss: -0.047070]\n",
      "[Epoch 12] [Batch 150/2000] [D loss: 1.948466] [G loss: -0.282728]\n",
      "[Epoch 12] [Batch 200/2000] [D loss: 1.724858] [G loss: -0.322298]\n",
      "[Epoch 12] [Batch 250/2000] [D loss: 2.121178] [G loss: -0.357385]\n",
      "[Epoch 12] [Batch 300/2000] [D loss: 1.670380] [G loss: -0.356342]\n",
      "[Epoch 12] [Batch 350/2000] [D loss: 2.228822] [G loss: -0.360381]\n",
      "[Epoch 12] [Batch 400/2000] [D loss: 1.796035] [G loss: -0.116238]\n",
      "[Epoch 12] [Batch 450/2000] [D loss: 1.743695] [G loss: -0.238026]\n",
      "[Epoch 12] [Batch 500/2000] [D loss: 1.710027] [G loss: 0.045439]\n",
      "[Epoch 12] [Batch 550/2000] [D loss: 1.336346] [G loss: 0.548820]\n",
      "[Epoch 12] [Batch 600/2000] [D loss: 1.588428] [G loss: 0.317690]\n",
      "[Epoch 12] [Batch 650/2000] [D loss: 2.010532] [G loss: 0.297024]\n",
      "[Epoch 12] [Batch 700/2000] [D loss: 2.281508] [G loss: -0.360997]\n",
      "[Epoch 12] [Batch 750/2000] [D loss: 1.896892] [G loss: -0.368544]\n",
      "[Epoch 12] [Batch 800/2000] [D loss: 1.032755] [G loss: 0.197518]\n",
      "[Epoch 12] [Batch 850/2000] [D loss: 1.723685] [G loss: 0.076806]\n",
      "[Epoch 12] [Batch 900/2000] [D loss: 2.264368] [G loss: -0.017406]\n",
      "[Epoch 12] [Batch 950/2000] [D loss: 2.018827] [G loss: 0.174591]\n",
      "[Epoch 12] [Batch 1000/2000] [D loss: 1.975060] [G loss: 0.017417]\n",
      "[Epoch 12] [Batch 1050/2000] [D loss: 2.057477] [G loss: -0.057669]\n",
      "[Epoch 12] [Batch 1100/2000] [D loss: 1.808033] [G loss: 0.024074]\n",
      "[Epoch 12] [Batch 1150/2000] [D loss: 1.783036] [G loss: -0.079727]\n",
      "[Epoch 12] [Batch 1200/2000] [D loss: 1.821476] [G loss: -0.157515]\n",
      "[Epoch 12] [Batch 1250/2000] [D loss: 1.698233] [G loss: -0.067038]\n",
      "[Epoch 12] [Batch 1300/2000] [D loss: 1.634582] [G loss: 0.126425]\n",
      "[Epoch 12] [Batch 1350/2000] [D loss: 1.260824] [G loss: 0.448817]\n",
      "[Epoch 12] [Batch 1400/2000] [D loss: 1.639416] [G loss: 0.213676]\n",
      "[Epoch 12] [Batch 1450/2000] [D loss: 2.156296] [G loss: -0.354611]\n",
      "[Epoch 12] [Batch 1500/2000] [D loss: 1.973607] [G loss: -0.281782]\n",
      "[Epoch 12] [Batch 1550/2000] [D loss: 2.105685] [G loss: -0.089621]\n",
      "[Epoch 12] [Batch 1600/2000] [D loss: 1.236504] [G loss: 0.232632]\n",
      "[Epoch 12] [Batch 1650/2000] [D loss: 1.619779] [G loss: -0.117606]\n",
      "[Epoch 12] [Batch 1700/2000] [D loss: 1.880817] [G loss: 0.032104]\n",
      "[Epoch 12] [Batch 1750/2000] [D loss: 1.544989] [G loss: -0.329477]\n",
      "[Epoch 12] [Batch 1800/2000] [D loss: 1.981587] [G loss: -0.209891]\n",
      "[Epoch 12] [Batch 1850/2000] [D loss: 2.347236] [G loss: 0.460297]\n",
      "[Epoch 12] [Batch 1900/2000] [D loss: 2.017071] [G loss: -0.488785]\n",
      "[Epoch 12] [Batch 1950/2000] [D loss: 2.081778] [G loss: -0.099557]\n",
      "[Epoch 12] [Batch 2000/2000] [D loss: 1.710033] [G loss: -0.328930]\n",
      "[Epoch 13] [Batch 50/2000] [D loss: 2.128667] [G loss: -0.442033]\n",
      "[Epoch 13] [Batch 100/2000] [D loss: 1.910744] [G loss: -0.392126]\n",
      "[Epoch 13] [Batch 150/2000] [D loss: 2.141685] [G loss: -0.583173]\n",
      "[Epoch 13] [Batch 200/2000] [D loss: 2.367082] [G loss: -0.489889]\n",
      "[Epoch 13] [Batch 250/2000] [D loss: 1.691146] [G loss: -0.030201]\n",
      "[Epoch 13] [Batch 300/2000] [D loss: 1.502196] [G loss: 0.354844]\n",
      "[Epoch 13] [Batch 350/2000] [D loss: 2.144699] [G loss: -0.529834]\n",
      "[Epoch 13] [Batch 400/2000] [D loss: 1.494057] [G loss: 0.143626]\n",
      "[Epoch 13] [Batch 450/2000] [D loss: 1.794900] [G loss: -0.187758]\n",
      "[Epoch 13] [Batch 500/2000] [D loss: 2.036291] [G loss: -0.001817]\n",
      "[Epoch 13] [Batch 550/2000] [D loss: 2.134595] [G loss: -0.671631]\n",
      "[Epoch 13] [Batch 600/2000] [D loss: 1.493340] [G loss: -0.105258]\n",
      "[Epoch 13] [Batch 650/2000] [D loss: 1.391377] [G loss: 0.012349]\n",
      "[Epoch 13] [Batch 700/2000] [D loss: 1.542578] [G loss: 0.560209]\n",
      "[Epoch 13] [Batch 750/2000] [D loss: 1.461955] [G loss: 0.347559]\n",
      "[Epoch 13] [Batch 800/2000] [D loss: 1.660921] [G loss: 0.010089]\n",
      "[Epoch 13] [Batch 850/2000] [D loss: 1.743987] [G loss: -0.269971]\n",
      "[Epoch 13] [Batch 900/2000] [D loss: 2.020253] [G loss: -0.263667]\n",
      "[Epoch 13] [Batch 950/2000] [D loss: 1.350036] [G loss: 0.284373]\n",
      "[Epoch 13] [Batch 1000/2000] [D loss: 1.924280] [G loss: -0.016496]\n",
      "[Epoch 13] [Batch 1050/2000] [D loss: 2.062146] [G loss: -0.253046]\n",
      "[Epoch 13] [Batch 1100/2000] [D loss: 1.508914] [G loss: 0.076317]\n",
      "[Epoch 13] [Batch 1150/2000] [D loss: 1.799938] [G loss: 0.035601]\n",
      "[Epoch 13] [Batch 1200/2000] [D loss: 1.958205] [G loss: -0.213219]\n",
      "[Epoch 13] [Batch 1250/2000] [D loss: 2.335993] [G loss: -0.098382]\n",
      "[Epoch 13] [Batch 1300/2000] [D loss: 1.562847] [G loss: -0.253021]\n",
      "[Epoch 13] [Batch 1350/2000] [D loss: 1.582918] [G loss: -0.129837]\n",
      "[Epoch 13] [Batch 1400/2000] [D loss: 1.829562] [G loss: -0.182635]\n",
      "[Epoch 13] [Batch 1450/2000] [D loss: 1.919268] [G loss: 0.049178]\n",
      "[Epoch 13] [Batch 1500/2000] [D loss: 1.883221] [G loss: -0.237581]\n",
      "[Epoch 13] [Batch 1550/2000] [D loss: 2.015669] [G loss: -0.175989]\n",
      "[Epoch 13] [Batch 1600/2000] [D loss: 2.296174] [G loss: -0.323776]\n",
      "[Epoch 13] [Batch 1650/2000] [D loss: 2.333519] [G loss: -0.552656]\n",
      "[Epoch 13] [Batch 1700/2000] [D loss: 2.108220] [G loss: -0.301312]\n",
      "[Epoch 13] [Batch 1750/2000] [D loss: 1.571596] [G loss: -0.254917]\n",
      "[Epoch 13] [Batch 1800/2000] [D loss: 1.889292] [G loss: -0.150998]\n",
      "[Epoch 13] [Batch 1850/2000] [D loss: 2.327397] [G loss: -0.453772]\n",
      "[Epoch 13] [Batch 1900/2000] [D loss: 1.869461] [G loss: -0.324516]\n",
      "[Epoch 13] [Batch 1950/2000] [D loss: 2.206225] [G loss: -0.237333]\n",
      "[Epoch 13] [Batch 2000/2000] [D loss: 2.034333] [G loss: -0.128328]\n",
      "[Epoch 14] [Batch 50/2000] [D loss: 1.903131] [G loss: -0.007957]\n",
      "[Epoch 14] [Batch 100/2000] [D loss: 1.983819] [G loss: -0.318444]\n",
      "[Epoch 14] [Batch 150/2000] [D loss: 1.991475] [G loss: -0.298526]\n",
      "[Epoch 14] [Batch 200/2000] [D loss: 2.138650] [G loss: -0.403938]\n",
      "[Epoch 14] [Batch 250/2000] [D loss: 2.093315] [G loss: -0.114371]\n",
      "[Epoch 14] [Batch 300/2000] [D loss: 1.653731] [G loss: 0.135079]\n",
      "[Epoch 14] [Batch 350/2000] [D loss: 2.186831] [G loss: -0.345398]\n",
      "[Epoch 14] [Batch 400/2000] [D loss: 1.862652] [G loss: -0.179888]\n",
      "[Epoch 14] [Batch 450/2000] [D loss: 2.129580] [G loss: -0.261958]\n",
      "[Epoch 14] [Batch 500/2000] [D loss: 1.902502] [G loss: -0.354177]\n",
      "[Epoch 14] [Batch 550/2000] [D loss: 2.167462] [G loss: -0.560955]\n",
      "[Epoch 14] [Batch 600/2000] [D loss: 1.847141] [G loss: -0.286929]\n",
      "[Epoch 14] [Batch 650/2000] [D loss: 1.947447] [G loss: -0.079037]\n",
      "[Epoch 14] [Batch 700/2000] [D loss: 1.907853] [G loss: -0.199501]\n",
      "[Epoch 14] [Batch 750/2000] [D loss: 1.982015] [G loss: -0.453796]\n",
      "[Epoch 14] [Batch 800/2000] [D loss: 1.968360] [G loss: 0.050124]\n",
      "[Epoch 14] [Batch 850/2000] [D loss: 1.939206] [G loss: -0.550032]\n",
      "[Epoch 14] [Batch 900/2000] [D loss: 1.925524] [G loss: -0.511337]\n",
      "[Epoch 14] [Batch 950/2000] [D loss: 2.271772] [G loss: -0.315372]\n",
      "[Epoch 14] [Batch 1000/2000] [D loss: 1.695422] [G loss: -0.051166]\n",
      "[Epoch 14] [Batch 1050/2000] [D loss: 1.498071] [G loss: -0.137952]\n",
      "[Epoch 14] [Batch 1100/2000] [D loss: 1.884948] [G loss: -0.455509]\n",
      "[Epoch 14] [Batch 1150/2000] [D loss: 1.911192] [G loss: -0.211863]\n",
      "[Epoch 14] [Batch 1200/2000] [D loss: 1.763779] [G loss: -0.517775]\n",
      "[Epoch 14] [Batch 1250/2000] [D loss: 1.991141] [G loss: -0.318399]\n",
      "[Epoch 14] [Batch 1300/2000] [D loss: 1.735515] [G loss: -0.256268]\n",
      "[Epoch 14] [Batch 1350/2000] [D loss: 1.769895] [G loss: -0.073363]\n",
      "[Epoch 14] [Batch 1400/2000] [D loss: 1.790503] [G loss: -0.221816]\n",
      "[Epoch 14] [Batch 1450/2000] [D loss: 2.216301] [G loss: -0.501088]\n",
      "[Epoch 14] [Batch 1500/2000] [D loss: 2.217285] [G loss: -0.368946]\n",
      "[Epoch 14] [Batch 1550/2000] [D loss: 1.933914] [G loss: -0.391261]\n",
      "[Epoch 14] [Batch 1600/2000] [D loss: 1.789999] [G loss: -0.042939]\n",
      "[Epoch 14] [Batch 1650/2000] [D loss: 1.589359] [G loss: -0.260098]\n",
      "[Epoch 14] [Batch 1700/2000] [D loss: 2.431427] [G loss: -0.504990]\n",
      "[Epoch 14] [Batch 1750/2000] [D loss: 1.959419] [G loss: -0.500578]\n",
      "[Epoch 14] [Batch 1800/2000] [D loss: 2.331126] [G loss: -0.373206]\n",
      "[Epoch 14] [Batch 1850/2000] [D loss: 1.929493] [G loss: -0.448443]\n",
      "[Epoch 14] [Batch 1900/2000] [D loss: 2.009636] [G loss: -0.405800]\n",
      "[Epoch 14] [Batch 1950/2000] [D loss: 2.230784] [G loss: -0.194572]\n",
      "[Epoch 14] [Batch 2000/2000] [D loss: 1.816298] [G loss: -0.181771]\n",
      "[Epoch 15] [Batch 50/2000] [D loss: 1.964066] [G loss: -0.076590]\n",
      "[Epoch 15] [Batch 100/2000] [D loss: 1.681905] [G loss: 0.248007]\n",
      "[Epoch 15] [Batch 150/2000] [D loss: 1.634803] [G loss: 0.044166]\n",
      "[Epoch 15] [Batch 200/2000] [D loss: 1.107672] [G loss: 0.785526]\n",
      "[Epoch 15] [Batch 250/2000] [D loss: 1.792837] [G loss: -0.223018]\n",
      "[Epoch 15] [Batch 300/2000] [D loss: 2.076427] [G loss: -0.304552]\n",
      "[Epoch 15] [Batch 350/2000] [D loss: 1.569008] [G loss: 0.113974]\n",
      "[Epoch 15] [Batch 400/2000] [D loss: 1.663523] [G loss: -0.204865]\n",
      "[Epoch 15] [Batch 450/2000] [D loss: 1.324356] [G loss: 0.252133]\n",
      "[Epoch 15] [Batch 500/2000] [D loss: 2.168960] [G loss: -0.171045]\n",
      "[Epoch 15] [Batch 550/2000] [D loss: 2.031909] [G loss: -0.267352]\n",
      "[Epoch 15] [Batch 600/2000] [D loss: 2.024088] [G loss: -0.333807]\n",
      "[Epoch 15] [Batch 650/2000] [D loss: 2.060707] [G loss: -0.212811]\n",
      "[Epoch 15] [Batch 700/2000] [D loss: 2.212049] [G loss: -0.373052]\n",
      "[Epoch 15] [Batch 750/2000] [D loss: 1.942105] [G loss: -0.199764]\n",
      "[Epoch 15] [Batch 800/2000] [D loss: 2.154775] [G loss: -0.215379]\n",
      "[Epoch 15] [Batch 850/2000] [D loss: 1.639417] [G loss: -0.069995]\n",
      "[Epoch 15] [Batch 900/2000] [D loss: 1.957303] [G loss: -0.055309]\n",
      "[Epoch 15] [Batch 950/2000] [D loss: 1.769758] [G loss: -0.332773]\n",
      "[Epoch 15] [Batch 1000/2000] [D loss: 1.898890] [G loss: -0.229829]\n",
      "[Epoch 15] [Batch 1050/2000] [D loss: 1.724585] [G loss: -0.023364]\n",
      "[Epoch 15] [Batch 1100/2000] [D loss: 1.747475] [G loss: -0.165446]\n",
      "[Epoch 15] [Batch 1150/2000] [D loss: 1.570010] [G loss: -0.024552]\n",
      "[Epoch 15] [Batch 1200/2000] [D loss: 1.993587] [G loss: -0.332495]\n",
      "[Epoch 15] [Batch 1250/2000] [D loss: 2.059623] [G loss: -0.449798]\n",
      "[Epoch 15] [Batch 1300/2000] [D loss: 2.107703] [G loss: -0.103187]\n",
      "[Epoch 15] [Batch 1350/2000] [D loss: 1.854472] [G loss: -0.342742]\n",
      "[Epoch 15] [Batch 1400/2000] [D loss: 1.685332] [G loss: -0.274047]\n",
      "[Epoch 15] [Batch 1450/2000] [D loss: 2.169606] [G loss: -0.383508]\n",
      "[Epoch 15] [Batch 1500/2000] [D loss: 1.861839] [G loss: -0.169328]\n",
      "[Epoch 15] [Batch 1550/2000] [D loss: 1.635284] [G loss: 0.067921]\n",
      "[Epoch 15] [Batch 1600/2000] [D loss: 1.688896] [G loss: -0.354036]\n",
      "[Epoch 15] [Batch 1650/2000] [D loss: 1.802925] [G loss: -0.215301]\n",
      "[Epoch 15] [Batch 1700/2000] [D loss: 1.990970] [G loss: -0.305618]\n",
      "[Epoch 15] [Batch 1750/2000] [D loss: 2.067140] [G loss: -0.261287]\n",
      "[Epoch 15] [Batch 1800/2000] [D loss: 2.144855] [G loss: -0.178137]\n",
      "[Epoch 15] [Batch 1850/2000] [D loss: 1.953070] [G loss: -0.256924]\n",
      "[Epoch 15] [Batch 1900/2000] [D loss: 1.794635] [G loss: -0.006230]\n",
      "[Epoch 15] [Batch 1950/2000] [D loss: 1.529538] [G loss: 0.496698]\n",
      "[Epoch 15] [Batch 2000/2000] [D loss: 2.465950] [G loss: 0.286862]\n",
      "[Epoch 16] [Batch 50/2000] [D loss: 1.932944] [G loss: -0.480122]\n",
      "[Epoch 16] [Batch 100/2000] [D loss: 1.885570] [G loss: -0.271445]\n",
      "[Epoch 16] [Batch 150/2000] [D loss: 1.970746] [G loss: -0.088719]\n",
      "[Epoch 16] [Batch 200/2000] [D loss: 2.203965] [G loss: -0.461914]\n",
      "[Epoch 16] [Batch 250/2000] [D loss: 2.020333] [G loss: -0.306240]\n",
      "[Epoch 16] [Batch 300/2000] [D loss: 1.738400] [G loss: 0.094758]\n",
      "[Epoch 16] [Batch 350/2000] [D loss: 1.510845] [G loss: 0.516949]\n",
      "[Epoch 16] [Batch 400/2000] [D loss: 1.732715] [G loss: -0.028742]\n",
      "[Epoch 16] [Batch 450/2000] [D loss: 2.190053] [G loss: -0.145094]\n",
      "[Epoch 16] [Batch 500/2000] [D loss: 1.992322] [G loss: -0.025733]\n",
      "[Epoch 16] [Batch 550/2000] [D loss: 2.006472] [G loss: -0.330821]\n",
      "[Epoch 16] [Batch 600/2000] [D loss: 2.024140] [G loss: -0.392161]\n",
      "[Epoch 16] [Batch 650/2000] [D loss: 2.006553] [G loss: -0.325772]\n",
      "[Epoch 16] [Batch 700/2000] [D loss: 1.889357] [G loss: -0.067477]\n",
      "[Epoch 16] [Batch 750/2000] [D loss: 1.689862] [G loss: 0.160631]\n",
      "[Epoch 16] [Batch 800/2000] [D loss: 1.744479] [G loss: -0.190309]\n",
      "[Epoch 16] [Batch 850/2000] [D loss: 1.970114] [G loss: 0.159285]\n",
      "[Epoch 16] [Batch 900/2000] [D loss: 1.851579] [G loss: -0.222945]\n",
      "[Epoch 16] [Batch 950/2000] [D loss: 2.188272] [G loss: -0.046486]\n",
      "[Epoch 16] [Batch 1000/2000] [D loss: 1.567666] [G loss: -0.290448]\n",
      "[Epoch 16] [Batch 1050/2000] [D loss: 2.232746] [G loss: -0.388007]\n",
      "[Epoch 16] [Batch 1100/2000] [D loss: 2.559793] [G loss: -0.217338]\n",
      "[Epoch 16] [Batch 1150/2000] [D loss: 2.288160] [G loss: -0.325408]\n",
      "[Epoch 16] [Batch 1200/2000] [D loss: 1.754373] [G loss: -0.290074]\n",
      "[Epoch 16] [Batch 1250/2000] [D loss: 1.815121] [G loss: -0.161078]\n",
      "[Epoch 16] [Batch 1300/2000] [D loss: 2.249383] [G loss: -0.256963]\n",
      "[Epoch 16] [Batch 1350/2000] [D loss: 2.175223] [G loss: -0.460391]\n",
      "[Epoch 16] [Batch 1400/2000] [D loss: 2.131826] [G loss: -0.345091]\n",
      "[Epoch 16] [Batch 1450/2000] [D loss: 1.825207] [G loss: -0.218780]\n",
      "[Epoch 16] [Batch 1500/2000] [D loss: 1.769313] [G loss: -0.090347]\n",
      "[Epoch 16] [Batch 1550/2000] [D loss: 1.763200] [G loss: -0.061688]\n",
      "[Epoch 16] [Batch 1600/2000] [D loss: 1.906825] [G loss: -0.419851]\n",
      "[Epoch 16] [Batch 1650/2000] [D loss: 1.856562] [G loss: -0.132225]\n",
      "[Epoch 16] [Batch 1700/2000] [D loss: 2.104398] [G loss: -0.369746]\n",
      "[Epoch 16] [Batch 1750/2000] [D loss: 2.487622] [G loss: -0.345612]\n",
      "[Epoch 16] [Batch 1800/2000] [D loss: 1.849100] [G loss: -0.206515]\n",
      "[Epoch 16] [Batch 1850/2000] [D loss: 2.013822] [G loss: -0.279157]\n",
      "[Epoch 16] [Batch 1900/2000] [D loss: 1.554347] [G loss: -0.040718]\n",
      "[Epoch 16] [Batch 1950/2000] [D loss: 1.977177] [G loss: -0.281570]\n",
      "[Epoch 16] [Batch 2000/2000] [D loss: 2.195926] [G loss: -0.415746]\n",
      "[Epoch 17] [Batch 50/2000] [D loss: 2.190735] [G loss: -0.272278]\n",
      "[Epoch 17] [Batch 100/2000] [D loss: 2.066173] [G loss: -0.318558]\n",
      "[Epoch 17] [Batch 150/2000] [D loss: 1.989260] [G loss: -0.424771]\n",
      "[Epoch 17] [Batch 200/2000] [D loss: 1.690914] [G loss: -0.349809]\n",
      "[Epoch 17] [Batch 250/2000] [D loss: 2.117871] [G loss: -0.450190]\n",
      "[Epoch 17] [Batch 300/2000] [D loss: 1.922229] [G loss: -0.362340]\n",
      "[Epoch 17] [Batch 350/2000] [D loss: 1.861879] [G loss: 0.020808]\n",
      "[Epoch 17] [Batch 400/2000] [D loss: 1.865798] [G loss: -0.300184]\n",
      "[Epoch 17] [Batch 450/2000] [D loss: 2.049398] [G loss: -0.104580]\n",
      "[Epoch 17] [Batch 500/2000] [D loss: 1.884817] [G loss: -0.146889]\n",
      "[Epoch 17] [Batch 550/2000] [D loss: 1.788247] [G loss: -0.094372]\n",
      "[Epoch 17] [Batch 600/2000] [D loss: 1.554307] [G loss: 0.035395]\n",
      "[Epoch 17] [Batch 650/2000] [D loss: 1.589020] [G loss: 0.003804]\n",
      "[Epoch 17] [Batch 700/2000] [D loss: 2.155460] [G loss: -0.114662]\n",
      "[Epoch 17] [Batch 750/2000] [D loss: 2.044863] [G loss: -0.290628]\n",
      "[Epoch 17] [Batch 800/2000] [D loss: 1.997421] [G loss: -0.116536]\n",
      "[Epoch 17] [Batch 850/2000] [D loss: 1.759846] [G loss: -0.311976]\n",
      "[Epoch 17] [Batch 900/2000] [D loss: 1.620297] [G loss: -0.167921]\n",
      "[Epoch 17] [Batch 950/2000] [D loss: 1.817845] [G loss: -0.209510]\n",
      "[Epoch 17] [Batch 1000/2000] [D loss: 1.889565] [G loss: -0.374445]\n",
      "[Epoch 17] [Batch 1050/2000] [D loss: 2.008376] [G loss: -0.243439]\n",
      "[Epoch 17] [Batch 1100/2000] [D loss: 1.648184] [G loss: -0.109698]\n",
      "[Epoch 17] [Batch 1150/2000] [D loss: 1.668015] [G loss: 0.003736]\n",
      "[Epoch 17] [Batch 1200/2000] [D loss: 1.863094] [G loss: -0.299390]\n",
      "[Epoch 17] [Batch 1250/2000] [D loss: 2.077825] [G loss: -0.222244]\n",
      "[Epoch 17] [Batch 1300/2000] [D loss: 2.483077] [G loss: -0.110739]\n",
      "[Epoch 17] [Batch 1350/2000] [D loss: 1.704608] [G loss: -0.264670]\n",
      "[Epoch 17] [Batch 1400/2000] [D loss: 1.832268] [G loss: -0.018306]\n",
      "[Epoch 17] [Batch 1450/2000] [D loss: 2.251853] [G loss: 0.067221]\n",
      "[Epoch 17] [Batch 1500/2000] [D loss: 2.256244] [G loss: -0.158994]\n",
      "[Epoch 17] [Batch 1550/2000] [D loss: 1.895927] [G loss: 0.094134]\n",
      "[Epoch 17] [Batch 1600/2000] [D loss: 1.863044] [G loss: 0.013507]\n",
      "[Epoch 17] [Batch 1650/2000] [D loss: 1.704178] [G loss: 0.272502]\n",
      "[Epoch 17] [Batch 1700/2000] [D loss: 2.044747] [G loss: -0.403738]\n",
      "[Epoch 17] [Batch 1750/2000] [D loss: 2.166117] [G loss: -0.315089]\n",
      "[Epoch 17] [Batch 1800/2000] [D loss: 2.005854] [G loss: -0.275065]\n",
      "[Epoch 17] [Batch 1850/2000] [D loss: 2.209667] [G loss: -0.111627]\n",
      "[Epoch 17] [Batch 1900/2000] [D loss: 1.931352] [G loss: 0.076723]\n",
      "[Epoch 17] [Batch 1950/2000] [D loss: 1.920516] [G loss: -0.435852]\n",
      "[Epoch 17] [Batch 2000/2000] [D loss: 1.735891] [G loss: -0.198071]\n",
      "[Epoch 18] [Batch 50/2000] [D loss: 2.302195] [G loss: 0.002173]\n",
      "[Epoch 18] [Batch 100/2000] [D loss: 1.778091] [G loss: 0.183091]\n",
      "[Epoch 18] [Batch 150/2000] [D loss: 1.693314] [G loss: 0.172826]\n",
      "[Epoch 18] [Batch 200/2000] [D loss: 1.834595] [G loss: 0.295601]\n",
      "[Epoch 18] [Batch 250/2000] [D loss: 1.612860] [G loss: 0.117965]\n",
      "[Epoch 18] [Batch 300/2000] [D loss: 1.712490] [G loss: 0.350575]\n",
      "[Epoch 18] [Batch 350/2000] [D loss: 1.585258] [G loss: 0.414839]\n",
      "[Epoch 18] [Batch 400/2000] [D loss: 1.733104] [G loss: 0.017703]\n",
      "[Epoch 18] [Batch 450/2000] [D loss: 2.074860] [G loss: -0.149192]\n",
      "[Epoch 18] [Batch 500/2000] [D loss: 1.737256] [G loss: -0.037178]\n",
      "[Epoch 18] [Batch 550/2000] [D loss: 2.441524] [G loss: -0.154910]\n",
      "[Epoch 18] [Batch 600/2000] [D loss: 1.985401] [G loss: 0.045327]\n",
      "[Epoch 18] [Batch 650/2000] [D loss: 1.911697] [G loss: -0.075213]\n",
      "[Epoch 18] [Batch 700/2000] [D loss: 2.604039] [G loss: -0.425651]\n",
      "[Epoch 18] [Batch 750/2000] [D loss: 2.255854] [G loss: -0.513583]\n",
      "[Epoch 18] [Batch 800/2000] [D loss: 2.345065] [G loss: -0.230745]\n",
      "[Epoch 18] [Batch 850/2000] [D loss: 2.630009] [G loss: -0.292004]\n",
      "[Epoch 18] [Batch 900/2000] [D loss: 1.901583] [G loss: -0.102777]\n",
      "[Epoch 18] [Batch 950/2000] [D loss: 1.778081] [G loss: 0.049679]\n",
      "[Epoch 18] [Batch 1000/2000] [D loss: 1.861054] [G loss: -0.112589]\n",
      "[Epoch 18] [Batch 1050/2000] [D loss: 1.717935] [G loss: 0.091676]\n",
      "[Epoch 18] [Batch 1100/2000] [D loss: 1.892796] [G loss: -0.290546]\n",
      "[Epoch 18] [Batch 1150/2000] [D loss: 1.835636] [G loss: 0.129758]\n",
      "[Epoch 18] [Batch 1200/2000] [D loss: 2.043189] [G loss: -0.087377]\n",
      "[Epoch 18] [Batch 1250/2000] [D loss: 2.010130] [G loss: -0.005403]\n",
      "[Epoch 18] [Batch 1300/2000] [D loss: 1.991194] [G loss: -0.234889]\n",
      "[Epoch 18] [Batch 1350/2000] [D loss: 1.843094] [G loss: -0.228134]\n",
      "[Epoch 18] [Batch 1400/2000] [D loss: 1.988515] [G loss: 0.033478]\n",
      "[Epoch 18] [Batch 1450/2000] [D loss: 1.785407] [G loss: 0.131530]\n",
      "[Epoch 18] [Batch 1500/2000] [D loss: 1.830288] [G loss: 0.005319]\n",
      "[Epoch 18] [Batch 1550/2000] [D loss: 1.688132] [G loss: 0.024911]\n",
      "[Epoch 18] [Batch 1600/2000] [D loss: 1.596835] [G loss: -0.068844]\n",
      "[Epoch 18] [Batch 1650/2000] [D loss: 1.499888] [G loss: 0.203305]\n",
      "[Epoch 18] [Batch 1700/2000] [D loss: 1.825947] [G loss: -0.161803]\n",
      "[Epoch 18] [Batch 1750/2000] [D loss: 1.734030] [G loss: 0.020650]\n",
      "[Epoch 18] [Batch 1800/2000] [D loss: 1.891923] [G loss: -0.122282]\n",
      "[Epoch 18] [Batch 1850/2000] [D loss: 2.118184] [G loss: 0.309646]\n",
      "[Epoch 18] [Batch 1900/2000] [D loss: 2.001344] [G loss: -0.117024]\n",
      "[Epoch 18] [Batch 1950/2000] [D loss: 2.001257] [G loss: -0.074976]\n",
      "[Epoch 18] [Batch 2000/2000] [D loss: 1.987052] [G loss: -0.050128]\n",
      "[Epoch 19] [Batch 50/2000] [D loss: 2.233599] [G loss: -0.065652]\n",
      "[Epoch 19] [Batch 100/2000] [D loss: 2.075159] [G loss: -0.187784]\n",
      "[Epoch 19] [Batch 150/2000] [D loss: 1.844215] [G loss: 0.079991]\n",
      "[Epoch 19] [Batch 200/2000] [D loss: 2.137711] [G loss: -0.364748]\n",
      "[Epoch 19] [Batch 250/2000] [D loss: 2.029640] [G loss: -0.106116]\n",
      "[Epoch 19] [Batch 300/2000] [D loss: 2.031360] [G loss: -0.397975]\n",
      "[Epoch 19] [Batch 350/2000] [D loss: 1.973221] [G loss: -0.088064]\n",
      "[Epoch 19] [Batch 400/2000] [D loss: 1.897751] [G loss: -0.109702]\n",
      "[Epoch 19] [Batch 450/2000] [D loss: 2.072937] [G loss: -0.326690]\n",
      "[Epoch 19] [Batch 500/2000] [D loss: 1.810441] [G loss: -0.198633]\n",
      "[Epoch 19] [Batch 550/2000] [D loss: 1.378596] [G loss: 0.146726]\n",
      "[Epoch 19] [Batch 600/2000] [D loss: 1.867551] [G loss: -0.092663]\n",
      "[Epoch 19] [Batch 650/2000] [D loss: 2.255912] [G loss: -0.475945]\n",
      "[Epoch 19] [Batch 700/2000] [D loss: 2.133446] [G loss: -0.220821]\n",
      "[Epoch 19] [Batch 750/2000] [D loss: 2.116839] [G loss: -0.182007]\n",
      "[Epoch 19] [Batch 800/2000] [D loss: 1.837658] [G loss: -0.429871]\n",
      "[Epoch 19] [Batch 850/2000] [D loss: 2.116272] [G loss: -0.277938]\n",
      "[Epoch 19] [Batch 900/2000] [D loss: 1.513601] [G loss: -0.217851]\n",
      "[Epoch 19] [Batch 950/2000] [D loss: 1.880258] [G loss: -0.158737]\n",
      "[Epoch 19] [Batch 1000/2000] [D loss: 1.509325] [G loss: -0.149104]\n",
      "[Epoch 19] [Batch 1050/2000] [D loss: 1.770643] [G loss: -0.114700]\n",
      "[Epoch 19] [Batch 1100/2000] [D loss: 2.097028] [G loss: -0.351259]\n",
      "[Epoch 19] [Batch 1150/2000] [D loss: 1.946793] [G loss: -0.447375]\n",
      "[Epoch 19] [Batch 1200/2000] [D loss: 1.713842] [G loss: -0.071682]\n",
      "[Epoch 19] [Batch 1250/2000] [D loss: 1.832023] [G loss: 0.176745]\n",
      "[Epoch 19] [Batch 1300/2000] [D loss: 1.662128] [G loss: -0.141991]\n",
      "[Epoch 19] [Batch 1350/2000] [D loss: 1.507447] [G loss: 0.028851]\n",
      "[Epoch 19] [Batch 1400/2000] [D loss: 1.744140] [G loss: -0.165333]\n",
      "[Epoch 19] [Batch 1450/2000] [D loss: 1.910973] [G loss: 0.093436]\n",
      "[Epoch 19] [Batch 1500/2000] [D loss: 1.993962] [G loss: -0.126644]\n",
      "[Epoch 19] [Batch 1550/2000] [D loss: 1.731900] [G loss: -0.049780]\n",
      "[Epoch 19] [Batch 1600/2000] [D loss: 1.655345] [G loss: -0.048954]\n",
      "[Epoch 19] [Batch 1650/2000] [D loss: 1.699508] [G loss: 0.135753]\n",
      "[Epoch 19] [Batch 1700/2000] [D loss: 1.762070] [G loss: -0.076384]\n",
      "[Epoch 19] [Batch 1750/2000] [D loss: 1.622819] [G loss: 0.129144]\n",
      "[Epoch 19] [Batch 1800/2000] [D loss: 1.929113] [G loss: -0.051253]\n",
      "[Epoch 19] [Batch 1850/2000] [D loss: 1.891903] [G loss: -0.113160]\n",
      "[Epoch 19] [Batch 1900/2000] [D loss: 1.779552] [G loss: -0.095772]\n",
      "[Epoch 19] [Batch 1950/2000] [D loss: 1.727111] [G loss: -0.068231]\n",
      "[Epoch 19] [Batch 2000/2000] [D loss: 2.178944] [G loss: 0.035535]\n",
      "[Epoch 20] [Batch 50/2000] [D loss: 2.107073] [G loss: -0.374827]\n",
      "[Epoch 20] [Batch 100/2000] [D loss: 1.930446] [G loss: -0.069964]\n",
      "[Epoch 20] [Batch 150/2000] [D loss: 1.982923] [G loss: 0.082299]\n",
      "[Epoch 20] [Batch 200/2000] [D loss: 2.058805] [G loss: 0.016496]\n",
      "[Epoch 20] [Batch 250/2000] [D loss: 1.758082] [G loss: 0.174484]\n",
      "[Epoch 20] [Batch 300/2000] [D loss: 2.081193] [G loss: -0.299852]\n",
      "[Epoch 20] [Batch 350/2000] [D loss: 2.113926] [G loss: -0.082397]\n",
      "[Epoch 20] [Batch 400/2000] [D loss: 2.049345] [G loss: -0.302265]\n",
      "[Epoch 20] [Batch 450/2000] [D loss: 2.046722] [G loss: -0.122202]\n",
      "[Epoch 20] [Batch 500/2000] [D loss: 2.089116] [G loss: -0.421308]\n",
      "[Epoch 20] [Batch 550/2000] [D loss: 2.042675] [G loss: -0.194731]\n",
      "[Epoch 20] [Batch 600/2000] [D loss: 2.101202] [G loss: -0.210502]\n",
      "[Epoch 20] [Batch 650/2000] [D loss: 2.242291] [G loss: -0.214217]\n",
      "[Epoch 20] [Batch 700/2000] [D loss: 1.947710] [G loss: -0.176510]\n",
      "[Epoch 20] [Batch 750/2000] [D loss: 1.940607] [G loss: -0.326584]\n",
      "[Epoch 20] [Batch 800/2000] [D loss: 2.264374] [G loss: -0.228775]\n",
      "[Epoch 20] [Batch 850/2000] [D loss: 1.851843] [G loss: -0.215973]\n",
      "[Epoch 20] [Batch 900/2000] [D loss: 1.997818] [G loss: -0.452221]\n",
      "[Epoch 20] [Batch 950/2000] [D loss: 2.133595] [G loss: -0.391541]\n",
      "[Epoch 20] [Batch 1000/2000] [D loss: 1.957741] [G loss: -0.397158]\n",
      "[Epoch 20] [Batch 1050/2000] [D loss: 2.285178] [G loss: -0.323459]\n",
      "[Epoch 20] [Batch 1100/2000] [D loss: 2.093667] [G loss: -0.313429]\n",
      "[Epoch 20] [Batch 1150/2000] [D loss: 1.997856] [G loss: -0.248340]\n",
      "[Epoch 20] [Batch 1200/2000] [D loss: 2.333353] [G loss: -0.297067]\n",
      "[Epoch 20] [Batch 1250/2000] [D loss: 2.354865] [G loss: -0.326721]\n",
      "[Epoch 20] [Batch 1300/2000] [D loss: 2.038962] [G loss: -0.441027]\n",
      "[Epoch 20] [Batch 1350/2000] [D loss: 2.077127] [G loss: -0.156320]\n",
      "[Epoch 20] [Batch 1400/2000] [D loss: 1.903036] [G loss: -0.261147]\n",
      "[Epoch 20] [Batch 1450/2000] [D loss: 2.101463] [G loss: -0.423343]\n",
      "[Epoch 20] [Batch 1500/2000] [D loss: 2.471359] [G loss: -0.270139]\n",
      "[Epoch 20] [Batch 1550/2000] [D loss: 2.123811] [G loss: -0.432029]\n",
      "[Epoch 20] [Batch 1600/2000] [D loss: 2.064793] [G loss: -0.341762]\n",
      "[Epoch 20] [Batch 1650/2000] [D loss: 1.963141] [G loss: -0.225290]\n",
      "[Epoch 20] [Batch 1700/2000] [D loss: 1.659321] [G loss: -0.065781]\n",
      "[Epoch 20] [Batch 1750/2000] [D loss: 1.984954] [G loss: -0.215261]\n",
      "[Epoch 20] [Batch 1800/2000] [D loss: 1.768710] [G loss: -0.204331]\n",
      "[Epoch 20] [Batch 1850/2000] [D loss: 2.147251] [G loss: -0.114818]\n",
      "[Epoch 20] [Batch 1900/2000] [D loss: 1.941938] [G loss: -0.177115]\n",
      "[Epoch 20] [Batch 1950/2000] [D loss: 1.915369] [G loss: 0.001575]\n",
      "[Epoch 20] [Batch 2000/2000] [D loss: 1.690599] [G loss: 0.191776]\n",
      "[Epoch 21] [Batch 50/2000] [D loss: 1.861578] [G loss: -0.074432]\n",
      "[Epoch 21] [Batch 100/2000] [D loss: 2.186611] [G loss: 0.107756]\n",
      "[Epoch 21] [Batch 150/2000] [D loss: 2.060863] [G loss: -0.261042]\n",
      "[Epoch 21] [Batch 200/2000] [D loss: 2.053264] [G loss: -0.079828]\n",
      "[Epoch 21] [Batch 250/2000] [D loss: 1.861429] [G loss: -0.152779]\n",
      "[Epoch 21] [Batch 300/2000] [D loss: 2.035472] [G loss: -0.228254]\n",
      "[Epoch 21] [Batch 350/2000] [D loss: 1.973487] [G loss: -0.295752]\n",
      "[Epoch 21] [Batch 400/2000] [D loss: 1.982108] [G loss: -0.150709]\n",
      "[Epoch 21] [Batch 450/2000] [D loss: 2.473691] [G loss: -0.289687]\n",
      "[Epoch 21] [Batch 500/2000] [D loss: 2.049574] [G loss: -0.586694]\n",
      "[Epoch 21] [Batch 550/2000] [D loss: 1.893249] [G loss: -0.198125]\n",
      "[Epoch 21] [Batch 600/2000] [D loss: 1.894853] [G loss: -0.087763]\n",
      "[Epoch 21] [Batch 650/2000] [D loss: 2.141847] [G loss: 0.000798]\n",
      "[Epoch 21] [Batch 700/2000] [D loss: 2.179231] [G loss: -0.449854]\n",
      "[Epoch 21] [Batch 750/2000] [D loss: 2.070274] [G loss: -0.395986]\n",
      "[Epoch 21] [Batch 800/2000] [D loss: 2.111610] [G loss: -0.400278]\n",
      "[Epoch 21] [Batch 850/2000] [D loss: 2.114358] [G loss: -0.418561]\n",
      "[Epoch 21] [Batch 900/2000] [D loss: 1.691595] [G loss: 0.034873]\n",
      "[Epoch 21] [Batch 950/2000] [D loss: 1.974754] [G loss: -0.249726]\n",
      "[Epoch 21] [Batch 1000/2000] [D loss: 1.567325] [G loss: -0.143645]\n",
      "[Epoch 21] [Batch 1050/2000] [D loss: 2.163996] [G loss: -0.249302]\n",
      "[Epoch 21] [Batch 1100/2000] [D loss: 2.071213] [G loss: 0.042315]\n",
      "[Epoch 21] [Batch 1150/2000] [D loss: 1.911449] [G loss: -0.334406]\n",
      "[Epoch 21] [Batch 1200/2000] [D loss: 2.210552] [G loss: -0.261760]\n",
      "[Epoch 21] [Batch 1250/2000] [D loss: 1.891842] [G loss: -0.329140]\n",
      "[Epoch 21] [Batch 1300/2000] [D loss: 1.913093] [G loss: -0.238296]\n",
      "[Epoch 21] [Batch 1350/2000] [D loss: 2.062523] [G loss: -0.316845]\n",
      "[Epoch 21] [Batch 1400/2000] [D loss: 2.002135] [G loss: -0.162018]\n",
      "[Epoch 21] [Batch 1450/2000] [D loss: 2.092637] [G loss: -0.219617]\n",
      "[Epoch 21] [Batch 1500/2000] [D loss: 1.640910] [G loss: 0.035635]\n",
      "[Epoch 21] [Batch 1550/2000] [D loss: 1.829750] [G loss: -0.155280]\n",
      "[Epoch 21] [Batch 1600/2000] [D loss: 1.840698] [G loss: 0.079993]\n",
      "[Epoch 21] [Batch 1650/2000] [D loss: 1.790959] [G loss: -0.144732]\n",
      "[Epoch 21] [Batch 1700/2000] [D loss: 2.333612] [G loss: -0.259885]\n",
      "[Epoch 21] [Batch 1750/2000] [D loss: 1.611010] [G loss: -0.120111]\n",
      "[Epoch 21] [Batch 1800/2000] [D loss: 2.003446] [G loss: -0.274598]\n",
      "[Epoch 21] [Batch 1850/2000] [D loss: 2.088183] [G loss: -0.221343]\n",
      "[Epoch 21] [Batch 1900/2000] [D loss: 1.996974] [G loss: -0.458471]\n",
      "[Epoch 21] [Batch 1950/2000] [D loss: 2.117006] [G loss: -0.384473]\n",
      "[Epoch 21] [Batch 2000/2000] [D loss: 1.714202] [G loss: -0.372839]\n",
      "[Epoch 22] [Batch 50/2000] [D loss: 2.125902] [G loss: -0.117728]\n",
      "[Epoch 22] [Batch 100/2000] [D loss: 2.028339] [G loss: -0.106686]\n",
      "[Epoch 22] [Batch 150/2000] [D loss: 1.719782] [G loss: -0.093923]\n",
      "[Epoch 22] [Batch 200/2000] [D loss: 1.758834] [G loss: -0.355092]\n",
      "[Epoch 22] [Batch 250/2000] [D loss: 1.904066] [G loss: -0.180293]\n",
      "[Epoch 22] [Batch 300/2000] [D loss: 2.206681] [G loss: -0.527320]\n",
      "[Epoch 22] [Batch 350/2000] [D loss: 1.821696] [G loss: -0.279751]\n",
      "[Epoch 22] [Batch 400/2000] [D loss: 2.094878] [G loss: -0.109359]\n",
      "[Epoch 22] [Batch 450/2000] [D loss: 1.738755] [G loss: -0.210636]\n",
      "[Epoch 22] [Batch 500/2000] [D loss: 1.802430] [G loss: -0.162041]\n",
      "[Epoch 22] [Batch 550/2000] [D loss: 1.966152] [G loss: -0.056431]\n",
      "[Epoch 22] [Batch 600/2000] [D loss: 1.694976] [G loss: -0.211913]\n",
      "[Epoch 22] [Batch 650/2000] [D loss: 2.159697] [G loss: -0.259878]\n",
      "[Epoch 22] [Batch 700/2000] [D loss: 2.250998] [G loss: -0.281007]\n",
      "[Epoch 22] [Batch 750/2000] [D loss: 2.076871] [G loss: -0.032252]\n",
      "[Epoch 22] [Batch 800/2000] [D loss: 1.921637] [G loss: -0.061248]\n",
      "[Epoch 22] [Batch 850/2000] [D loss: 1.814382] [G loss: 0.065453]\n",
      "[Epoch 22] [Batch 900/2000] [D loss: 2.198367] [G loss: -0.360372]\n",
      "[Epoch 22] [Batch 950/2000] [D loss: 1.934881] [G loss: -0.062576]\n",
      "[Epoch 22] [Batch 1000/2000] [D loss: 1.953204] [G loss: -0.264144]\n",
      "[Epoch 22] [Batch 1050/2000] [D loss: 1.976775] [G loss: -0.301612]\n",
      "[Epoch 22] [Batch 1100/2000] [D loss: 1.622919] [G loss: -0.256567]\n",
      "[Epoch 22] [Batch 1150/2000] [D loss: 1.919708] [G loss: -0.203819]\n",
      "[Epoch 22] [Batch 1200/2000] [D loss: 1.802229] [G loss: -0.083484]\n",
      "[Epoch 22] [Batch 1250/2000] [D loss: 1.990180] [G loss: -0.054207]\n",
      "[Epoch 22] [Batch 1300/2000] [D loss: 2.153721] [G loss: 0.237390]\n",
      "[Epoch 22] [Batch 1350/2000] [D loss: 2.097206] [G loss: -0.218401]\n",
      "[Epoch 22] [Batch 1400/2000] [D loss: 1.761282] [G loss: -0.141931]\n",
      "[Epoch 22] [Batch 1450/2000] [D loss: 2.042813] [G loss: -0.136785]\n",
      "[Epoch 22] [Batch 1500/2000] [D loss: 2.129838] [G loss: -0.332852]\n",
      "[Epoch 22] [Batch 1550/2000] [D loss: 2.072815] [G loss: -0.334558]\n",
      "[Epoch 22] [Batch 1600/2000] [D loss: 1.835629] [G loss: -0.264671]\n",
      "[Epoch 22] [Batch 1650/2000] [D loss: 1.777826] [G loss: -0.145729]\n",
      "[Epoch 22] [Batch 1700/2000] [D loss: 2.188539] [G loss: -0.259525]\n",
      "[Epoch 22] [Batch 1750/2000] [D loss: 2.141233] [G loss: 0.069719]\n",
      "[Epoch 22] [Batch 1800/2000] [D loss: 1.776552] [G loss: -0.181995]\n",
      "[Epoch 22] [Batch 1850/2000] [D loss: 1.895398] [G loss: -0.178231]\n",
      "[Epoch 22] [Batch 1900/2000] [D loss: 1.951667] [G loss: -0.270676]\n",
      "[Epoch 22] [Batch 1950/2000] [D loss: 2.339899] [G loss: -0.191336]\n",
      "[Epoch 22] [Batch 2000/2000] [D loss: 2.128912] [G loss: -0.315583]\n",
      "[Epoch 23] [Batch 50/2000] [D loss: 2.056339] [G loss: -0.248008]\n",
      "[Epoch 23] [Batch 100/2000] [D loss: 1.915845] [G loss: -0.346308]\n",
      "[Epoch 23] [Batch 150/2000] [D loss: 1.990884] [G loss: -0.218731]\n",
      "[Epoch 23] [Batch 200/2000] [D loss: 2.397328] [G loss: -0.441679]\n",
      "[Epoch 23] [Batch 250/2000] [D loss: 1.883492] [G loss: -0.292269]\n",
      "[Epoch 23] [Batch 300/2000] [D loss: 2.026912] [G loss: -0.154436]\n",
      "[Epoch 23] [Batch 350/2000] [D loss: 2.031856] [G loss: -0.390661]\n",
      "[Epoch 23] [Batch 400/2000] [D loss: 1.851029] [G loss: -0.256977]\n",
      "[Epoch 23] [Batch 450/2000] [D loss: 1.930772] [G loss: -0.350268]\n",
      "[Epoch 23] [Batch 500/2000] [D loss: 1.826675] [G loss: -0.207761]\n",
      "[Epoch 23] [Batch 550/2000] [D loss: 1.976476] [G loss: -0.198367]\n",
      "[Epoch 23] [Batch 600/2000] [D loss: 1.930449] [G loss: -0.028619]\n",
      "[Epoch 23] [Batch 650/2000] [D loss: 1.612817] [G loss: -0.154513]\n",
      "[Epoch 23] [Batch 700/2000] [D loss: 1.984077] [G loss: -0.177216]\n",
      "[Epoch 23] [Batch 750/2000] [D loss: 1.984017] [G loss: -0.150571]\n",
      "[Epoch 23] [Batch 800/2000] [D loss: 1.944537] [G loss: -0.192817]\n",
      "[Epoch 23] [Batch 850/2000] [D loss: 2.103652] [G loss: 0.022943]\n",
      "[Epoch 23] [Batch 900/2000] [D loss: 1.879678] [G loss: -0.102364]\n",
      "[Epoch 23] [Batch 950/2000] [D loss: 1.897460] [G loss: -0.095800]\n",
      "[Epoch 23] [Batch 1000/2000] [D loss: 2.083598] [G loss: -0.240951]\n",
      "[Epoch 23] [Batch 1050/2000] [D loss: 2.110967] [G loss: -0.079117]\n",
      "[Epoch 23] [Batch 1100/2000] [D loss: 2.019774] [G loss: -0.299033]\n",
      "[Epoch 23] [Batch 1150/2000] [D loss: 2.102849] [G loss: -0.197485]\n",
      "[Epoch 23] [Batch 1200/2000] [D loss: 2.008392] [G loss: -0.283437]\n",
      "[Epoch 23] [Batch 1250/2000] [D loss: 2.149388] [G loss: -0.028214]\n",
      "[Epoch 23] [Batch 1300/2000] [D loss: 1.933284] [G loss: -0.304740]\n",
      "[Epoch 23] [Batch 1350/2000] [D loss: 1.831189] [G loss: -0.271124]\n",
      "[Epoch 23] [Batch 1400/2000] [D loss: 1.987993] [G loss: -0.301210]\n",
      "[Epoch 23] [Batch 1450/2000] [D loss: 1.774817] [G loss: -0.340031]\n",
      "[Epoch 23] [Batch 1500/2000] [D loss: 2.123235] [G loss: -0.173007]\n",
      "[Epoch 23] [Batch 1550/2000] [D loss: 2.008266] [G loss: -0.362420]\n",
      "[Epoch 23] [Batch 1600/2000] [D loss: 2.056153] [G loss: -0.201694]\n",
      "[Epoch 23] [Batch 1650/2000] [D loss: 1.890138] [G loss: -0.230699]\n",
      "[Epoch 23] [Batch 1700/2000] [D loss: 1.870595] [G loss: 0.155659]\n",
      "[Epoch 23] [Batch 1750/2000] [D loss: 1.749745] [G loss: -0.070556]\n",
      "[Epoch 23] [Batch 1800/2000] [D loss: 1.915704] [G loss: -0.086822]\n",
      "[Epoch 23] [Batch 1850/2000] [D loss: 1.903086] [G loss: -0.325597]\n",
      "[Epoch 23] [Batch 1900/2000] [D loss: 1.920376] [G loss: 0.004661]\n",
      "[Epoch 23] [Batch 1950/2000] [D loss: 2.178893] [G loss: -0.248207]\n",
      "[Epoch 23] [Batch 2000/2000] [D loss: 1.692784] [G loss: -0.307359]\n",
      "[Epoch 24] [Batch 50/2000] [D loss: 1.795154] [G loss: -0.355294]\n",
      "[Epoch 24] [Batch 100/2000] [D loss: 2.158238] [G loss: -0.114381]\n",
      "[Epoch 24] [Batch 150/2000] [D loss: 2.000685] [G loss: 0.117667]\n",
      "[Epoch 24] [Batch 200/2000] [D loss: 1.992338] [G loss: -0.209839]\n",
      "[Epoch 24] [Batch 250/2000] [D loss: 2.117853] [G loss: -0.123113]\n",
      "[Epoch 24] [Batch 300/2000] [D loss: 1.888434] [G loss: 0.031259]\n",
      "[Epoch 24] [Batch 350/2000] [D loss: 2.033913] [G loss: -0.039966]\n",
      "[Epoch 24] [Batch 400/2000] [D loss: 1.787997] [G loss: 0.059548]\n",
      "[Epoch 24] [Batch 450/2000] [D loss: 2.107725] [G loss: -0.157812]\n",
      "[Epoch 24] [Batch 500/2000] [D loss: 1.990691] [G loss: -0.353822]\n",
      "[Epoch 24] [Batch 550/2000] [D loss: 2.027685] [G loss: -0.054727]\n",
      "[Epoch 24] [Batch 600/2000] [D loss: 1.574076] [G loss: -0.218650]\n",
      "[Epoch 24] [Batch 650/2000] [D loss: 1.961690] [G loss: 0.199994]\n",
      "[Epoch 24] [Batch 700/2000] [D loss: 1.844371] [G loss: -0.349255]\n",
      "[Epoch 24] [Batch 750/2000] [D loss: 2.207004] [G loss: -0.255578]\n",
      "[Epoch 24] [Batch 800/2000] [D loss: 2.004127] [G loss: -0.345283]\n",
      "[Epoch 24] [Batch 850/2000] [D loss: 1.945948] [G loss: 0.092421]\n",
      "[Epoch 24] [Batch 900/2000] [D loss: 2.205313] [G loss: -0.200350]\n",
      "[Epoch 24] [Batch 950/2000] [D loss: 2.172090] [G loss: 0.104509]\n",
      "[Epoch 24] [Batch 1000/2000] [D loss: 1.995491] [G loss: 0.101392]\n",
      "[Epoch 24] [Batch 1050/2000] [D loss: 1.931280] [G loss: 0.264811]\n",
      "[Epoch 24] [Batch 1100/2000] [D loss: 1.913610] [G loss: -0.115306]\n",
      "[Epoch 24] [Batch 1150/2000] [D loss: 2.038109] [G loss: -0.108662]\n",
      "[Epoch 24] [Batch 1200/2000] [D loss: 1.815108] [G loss: -0.070230]\n",
      "[Epoch 24] [Batch 1250/2000] [D loss: 2.039618] [G loss: -0.272452]\n",
      "[Epoch 24] [Batch 1300/2000] [D loss: 2.241378] [G loss: -0.033898]\n",
      "[Epoch 24] [Batch 1350/2000] [D loss: 2.016204] [G loss: -0.216029]\n",
      "[Epoch 24] [Batch 1400/2000] [D loss: 1.616221] [G loss: 0.251045]\n",
      "[Epoch 24] [Batch 1450/2000] [D loss: 1.810813] [G loss: 0.187933]\n",
      "[Epoch 24] [Batch 1500/2000] [D loss: 1.841376] [G loss: 0.354036]\n",
      "[Epoch 24] [Batch 1550/2000] [D loss: 1.632826] [G loss: -0.290694]\n",
      "[Epoch 24] [Batch 1600/2000] [D loss: 1.793329] [G loss: 0.152320]\n",
      "[Epoch 24] [Batch 1650/2000] [D loss: 1.790657] [G loss: -0.127321]\n",
      "[Epoch 24] [Batch 1700/2000] [D loss: 1.840769] [G loss: -0.032205]\n",
      "[Epoch 24] [Batch 1750/2000] [D loss: 2.138504] [G loss: -0.080509]\n",
      "[Epoch 24] [Batch 1800/2000] [D loss: 2.086315] [G loss: 0.097504]\n",
      "[Epoch 24] [Batch 1850/2000] [D loss: 1.911341] [G loss: 0.178552]\n",
      "[Epoch 24] [Batch 1900/2000] [D loss: 1.807299] [G loss: 0.394137]\n",
      "[Epoch 24] [Batch 1950/2000] [D loss: 2.010980] [G loss: -0.007586]\n",
      "[Epoch 24] [Batch 2000/2000] [D loss: 1.914356] [G loss: -0.125815]\n",
      "[Epoch 25] [Batch 50/2000] [D loss: 2.096936] [G loss: 0.049916]\n",
      "[Epoch 25] [Batch 100/2000] [D loss: 1.926465] [G loss: -0.114982]\n",
      "[Epoch 25] [Batch 150/2000] [D loss: 1.740529] [G loss: -0.220916]\n",
      "[Epoch 25] [Batch 200/2000] [D loss: 2.012604] [G loss: -0.119912]\n",
      "[Epoch 25] [Batch 250/2000] [D loss: 2.050075] [G loss: -0.020439]\n",
      "[Epoch 25] [Batch 300/2000] [D loss: 1.897973] [G loss: -0.129682]\n",
      "[Epoch 25] [Batch 350/2000] [D loss: 2.027298] [G loss: 0.147894]\n",
      "[Epoch 25] [Batch 400/2000] [D loss: 2.123561] [G loss: -0.092185]\n",
      "[Epoch 25] [Batch 450/2000] [D loss: 1.902592] [G loss: -0.273615]\n",
      "[Epoch 25] [Batch 500/2000] [D loss: 1.883365] [G loss: 0.013759]\n",
      "[Epoch 25] [Batch 550/2000] [D loss: 1.807289] [G loss: -0.155952]\n",
      "[Epoch 25] [Batch 600/2000] [D loss: 1.807817] [G loss: -0.016634]\n",
      "[Epoch 25] [Batch 650/2000] [D loss: 1.971856] [G loss: -0.179577]\n",
      "[Epoch 25] [Batch 700/2000] [D loss: 1.943248] [G loss: -0.087619]\n",
      "[Epoch 25] [Batch 750/2000] [D loss: 2.122196] [G loss: -0.048356]\n",
      "[Epoch 25] [Batch 800/2000] [D loss: 1.898512] [G loss: -0.135896]\n",
      "[Epoch 25] [Batch 850/2000] [D loss: 1.919065] [G loss: -0.184180]\n",
      "[Epoch 25] [Batch 900/2000] [D loss: 1.946738] [G loss: -0.117425]\n",
      "[Epoch 25] [Batch 950/2000] [D loss: 2.012453] [G loss: -0.338625]\n",
      "[Epoch 25] [Batch 1000/2000] [D loss: 1.831829] [G loss: -0.181301]\n",
      "[Epoch 25] [Batch 1050/2000] [D loss: 1.893671] [G loss: -0.043545]\n",
      "[Epoch 25] [Batch 1100/2000] [D loss: 1.950360] [G loss: -0.195753]\n",
      "[Epoch 25] [Batch 1150/2000] [D loss: 1.971044] [G loss: -0.182273]\n",
      "[Epoch 25] [Batch 1200/2000] [D loss: 1.781153] [G loss: 0.463747]\n",
      "[Epoch 25] [Batch 1250/2000] [D loss: 2.166477] [G loss: -0.050582]\n",
      "[Epoch 25] [Batch 1300/2000] [D loss: 2.192848] [G loss: 0.177243]\n",
      "[Epoch 25] [Batch 1350/2000] [D loss: 2.275152] [G loss: -0.160099]\n",
      "[Epoch 25] [Batch 1400/2000] [D loss: 1.426382] [G loss: 0.308575]\n",
      "[Epoch 25] [Batch 1450/2000] [D loss: 2.110331] [G loss: -0.190574]\n",
      "[Epoch 25] [Batch 1500/2000] [D loss: 1.949423] [G loss: -0.234799]\n",
      "[Epoch 25] [Batch 1550/2000] [D loss: 2.062640] [G loss: -0.062216]\n",
      "[Epoch 25] [Batch 1600/2000] [D loss: 2.062611] [G loss: -0.103811]\n",
      "[Epoch 25] [Batch 1650/2000] [D loss: 1.913687] [G loss: 0.073728]\n",
      "[Epoch 25] [Batch 1700/2000] [D loss: 1.831340] [G loss: -0.042570]\n",
      "[Epoch 25] [Batch 1750/2000] [D loss: 1.737163] [G loss: 0.132382]\n",
      "[Epoch 25] [Batch 1800/2000] [D loss: 1.827411] [G loss: -0.144811]\n",
      "[Epoch 25] [Batch 1850/2000] [D loss: 2.096858] [G loss: 0.035757]\n",
      "[Epoch 25] [Batch 1900/2000] [D loss: 1.977997] [G loss: 0.062163]\n",
      "[Epoch 25] [Batch 1950/2000] [D loss: 1.810161] [G loss: 0.036985]\n",
      "[Epoch 25] [Batch 2000/2000] [D loss: 1.904762] [G loss: 0.056471]\n"
     ]
    }
   ],
   "source": [
    "epoch = 25\n",
    "for epoch in range(epoch):\n",
    "\n",
    "    train(generator, discriminator, optim_gen, optim_dis,\n",
    "    epoch, writer,img_size=28, latent_dim = 128, gener_batch_size=60)\n",
    "\n",
    "    checkpoint = {'epoch':epoch}\n",
    "    checkpoint['generator_state_dict'] = generator.state_dict()\n",
    "    checkpoint['discriminator_state_dict'] = discriminator.state_dict()\n",
    "\n",
    "    score = validate(generator, writer_dict)\n",
    "\n",
    "checkpoint = {'epoch':epoch}\n",
    "checkpoint['generator_state_dict'] = generator.state_dict()\n",
    "checkpoint['discriminator_state_dict'] = discriminator.state_dict()\n",
    "score = validate(generator, writer_dict) \n",
    "save_checkpoint(checkpoint, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjN3CUMwe3qT"
   },
   "source": [
    "## Results \n",
    "\n",
    "In this paper, we aim to train the smallest model which was TransGAN-S on CIFAR-10 but since it takes long and we don't have enough computational power our qualitative and quantitative results were not so good. Therefore, we simply reduce the model like shallow model, and train it on the MNIST dataset. Although, this implemented model was so small when comparing DCGAN, the training takes long and the qualitative results were not so good comraing the convolutional based models. In this re-implementation, as some of the challenges that has been encountered, we could not be reproduced and  will discuss the reasons in other section. This is the qualitative results our implemetation on MNIST dataset: \n",
    "\n",
    "![f1](https://s3.gifyu.com/images/transgan_mnist1.gif) ![f2](https://s3.gifyu.com/images/transgan_mnist2.gif) \n",
    "\n",
    "In this implementation we could not use metrics such as FID or IS score since in the original paper there is no any benchmarking on MNIST dataset. You can try yourself with pretrained on MNIST at ```./checkpoint``` Also, training the model is relatively takes long time comparing the convoltional based GANs and we provide an alternative which is in the ```./cifar``` path to readers to look original benchmark.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPpaGlFAq4gh"
   },
   "source": [
    "## Challenges and Discussions\n",
    "\n",
    "This GAN model was implemented completely free of Convolutions (just in unflatten step, we use Conv2d) and used Transformers architectures which became popular since Vision Transformers, [ViT paper](https://arxiv.org/abs/2010.11929).\n",
    " \n",
    "\n",
    "During the implementation, there were some challenges that make it unable to solve and get the desired or aimed results. One of them is that the authors did not give detailed hyperparameters for each Transformers Blok and Multi-Head Attention Mechanism. Also, in the training part they did not give detailed of hyperparameters such as droprate, weight decay or batch normalization. \n",
    "\n",
    "Unlike the paper authors claims, Transformers based generator is not such as \"friendly memory\" generator which use the [Upsampling](https://arxiv.org/pdf/1609.05158.pdf) strategy to reduce the dimenson of channels and increse the dimension of H,W and patch embedding in ViT. Since when we try to train implementation for CIFAR-10 at Google Colab, we could not train it in more iteration. \n",
    "\n",
    "Therefore, when implementing and trainig this model, it shows that this model is relatively huge comparing the convolutional based models and to train it you more computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "```\n",
    "@article{jiang2021transgan,\n",
    "  title={TransGAN: Two Transformers Can Make One Strong GAN},\n",
    "  author={Jiang, Yifan and Chang, Shiyu and Wang, Zhangyang},\n",
    "  journal={arXiv preprint arXiv:2102.07074},\n",
    "  year={2021}\n",
    "}\n",
    "```\n",
    "```\n",
    "@article{dosovitskiy2020,\n",
    "  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n",
    "  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\n",
    "  journal={arXiv preprint arXiv:2010.11929},\n",
    "  year={2020}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
